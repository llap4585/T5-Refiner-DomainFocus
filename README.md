# T5-Refiner-DomainFocus
# T5-é¢„å¤„ç†ç‰¹å®šè¯è¯­é®ç…§å¢å¼º

![Views](https://komarev.com/ghpvc/?username=llap4585&repo=T5-Refiner-DomainFocus&label=Project%20Views&color=blue&style=flat-square)

---
<a name="Introduction"></a>
## Introduction
[â­ï¸English](#english) | [â­ï¸ä¸­æ–‡](#chinese)


[æ—¥æœ¬èª](#japanese) | [Deutsch](#deutsch) | [FranÃ§ais](#francais) | [EspaÃ±ol](#espanol) | [à¤¹à¤¿à¤¨à¥à¤¦à¥€](#hindi) | [í•œêµ­ì–´](#korean) | [PortuguÃªs](#portuguese)

---

[Demo](#Demo) 

[Requirements](#Requirements)

[References](#References)

[Privacy](#Privacy)

---
<a name="english"></a>
## â­ï¸ English

**T5-Refiner-DomainFocus** aims to empower models with intrinsic "semantic resilience" through **pre-training stage** strategy optimization, **enabling more robust handling of text corruption and the injection of domain-specific expertise.**

### ğŸ“– Project Background
During **the digitization of medical records**, **OCR (Optical Character Recognition)** often suffers from "character defects" in core terminology due to damaged paper, stamp occlusion, or other physical factors.
Traditional **T5** or **mT5** models (collectively referred to as T5) face two major challenges when processing such corrupted text:
* **Limitations of Random Masking**: The model learns to "guess" words based on sub-word roots rather than truly understanding complete medical concepts.
* **Tokenization Misalignment**: When letters are missing from a term, the tokenizer breaks it into meaningless fragments, causing the model to lose its semantic focus.

### âœ… Core Features
This project does not rely on complex hard-coded rules. Instead, it enhances model capabilities through optimized data preprocessing, masking strategies, and execution workflows:

**Note:** This project uses the T5 tokenizer itself.  

* **Generating training chunks from long texts based on paragraphs and punctuation**  
  Automatically constructs samples from novels, documents, or domain-specific corpora, splitting text based on paragraph boundaries and both Chinese and English punctuation. The last sentence of the previous text block is retained as a context prefix to ensure semantic continuity, mitigate cross-block semantic breaks, and constrain sample length to fit the model's context window.

* **Atomic masking guided by an expert vocabulary**  
  Using a custom domain-specific vocabulary, professional terms (e.g., *Acute Anterior Wall Myocardial Infarction*, *Percutaneous Coronary Intervention*) are masked as indivisible units. This prevents character-level shortcuts and forces the model to rely on contextual semantics and domain logic for reconstruction.

* **Manually controllable enhanced masking strategy**  
  Supports increasing the masking probability for high-difficulty terms (**ğŸ’¡ recommended 50%â€“70%, not exceeding 80%**) while simultaneously adjusting the overall mask ratio (around 20%â€“25%), enabling targeted reinforcement of the model's reasoning ability on weaker knowledge points.

* **Automatic avoidance of punctuation and non-semantic tokens**  
  Masking automatically skips Chinese and English punctuation, symbols, and tokenizer unknown characters (`<unk>`), ensuring that every masked span corresponds to a meaningful semantic unit.

* **Automatic merging of consecutive or overlapping spans**  
  Adjacent or overlapping mask spans are automatically merged to maintain the correct order of `<extra_id_n>` input/output pairs, ensuring continuous training signals and full compatibility with T5's original span corruption training methodology.

* **Multithreaded parallel processing and automated sample aggregation**  
  Supports adjustable thread counts for parallel sample generation, significantly improving efficiency on large-scale corpora. All generated samples are automatically aggregated in their original order and output as a standardized JSONL dataset ready for direct training.

**By combining context-aware chunking, keyword-prioritized masking, random span corruption, and artificially created "extreme information loss" scenarios, the model is trained to maintain accurate understanding and reconstruction of professional semantics even under highly challenging input conditions.**

> Applicable scenarios: medical, legal, technical documentation, or any domain requiring precise semantic understanding.

### â—ï¸ Training Notes
* **Preventing Early Stopping**: After preprocessing, T5 models may exhibit slow loss reduction or local fluctuations, which can trick systems into stopping training prematurely.
* **Convergence Judgment**: It is recommended to extend training duration and evaluate convergence based on whether the loss decreases steadily across multiple stages. Insufficient training will significantly degrade restoration performance.

### ğŸ“Š Evaluation
Based on preliminary testing with the mT5-base standard model:
* **Standard Model Performance**: The restoration rate for specialized terminology is estimated to be below 60%. The remaining 40% of results are often logically incoherent and unacceptable for professional use.
* **With DomainFocus Improvement**: The estimated restoration rate reaches 85%. Of the remaining 15% error margin, most are semantic synonyms, which greatly improves the overall readability and logical consistency of the text.

### âš ï¸ Limitations
* **Context Fragmentation**: Due to the limited sequence length and the restricted number of masks per segment, long documents may suffer from semantic disconnection during chunking. It is recommended to feed partial overlapping context back during re-training.
* **Algorithmic Limits**: Since T5 restoration is based on statistical probability, it is impossible to guarantee 100% accuracy when dealing with highly complex text.
* **Domain Dependency**: The restoration effectiveness is highly dependent on the coverage and depth of the predefined expert lexicon.

### ğŸŒŒ Future Roadmap
* Automatic Defect Sensing:
Utilizing "tokenization fragments" as implicit signals. When OCR recognition is severely misaligned, the model will automatically locate semantic ruptures via anomalies in the token sequence.
* Semantic Auto-Alignment:
Eliminating the need for manual anchor points to achieve end-to-end restoration of OCR-damaged text.

[Demo](#Demo)

---
<a name="chinese"></a>
## â­ï¸ä¸­æ–‡
**T5-Refiner-DomainFocus** æ—¨åœ¨é€šè¿‡**é¢„è®­ç»ƒé˜¶æ®µ**çš„ç­–ç•¥ä¼˜åŒ–ï¼Œèµ‹äºˆæ¨¡å‹ä¸€ç§å†…åœ¨çš„â€œè¯­ä¹‰éŸ§æ€§â€ï¼Œ**ä½¿å…¶èƒ½æ›´ç¨³å¥åœ°å¤„ç†æ–‡æœ¬ç¼ºæŸå’Œæ³¨å…¥é¢†åŸŸä¸“ä¸šçŸ¥è¯†ã€‚**

### ğŸ“–é¡¹ç›®èƒŒæ™¯
åœ¨å¤„ç†**åŒ»å­¦æ¡£æ¡ˆæ•°å­—åŒ–æ—¶**ï¼Œ**OCRï¼ˆå…‰å­¦å­—ç¬¦è¯†åˆ«ï¼‰** å¸¸å› çº¸è´¨å—æŸã€å°ç« é®æŒ¡ç­‰åŸå› ï¼Œå¯¼è‡´æ ¸å¿ƒæœ¯è¯­å‡ºç°â€œå­—ç¬¦ç¼ºæŸâ€ã€‚
ä¼ ç»Ÿçš„ **T5** æˆ– **mT5** æ¨¡å‹(ç»Ÿç§°T5ï¼‰åœ¨å¤„ç†è¿™äº›å—æŸæ–‡æœ¬æ—¶å­˜åœ¨ä¸¤ä¸ªä¸»è¦é—®é¢˜ï¼š
* **éšæœºé®è”½çš„å±€é™æ€§**ï¼šå¯¼è‡´æ¨¡å‹åªå­¦ä¼šäº†æ ¹æ®è¯æ ¹â€œçŒœè¯â€ï¼Œè€Œæ²¡æœ‰çœŸæ­£ç†è§£å®Œæ•´çš„åŒ»å­¦æ¦‚å¿µã€‚
* **åˆ†è¯é”™ä½é—®é¢˜**ï¼šå½“æœ¯è¯­ä¸¢å¤±å­—æ¯æ—¶ï¼Œåˆ†è¯å™¨ä¼šå°†å…¶åˆ‡ç¢ä¸ºæ— æ„ä¹‰çš„ç¢ç‰‡ï¼Œå¯¼è‡´æ¨¡å‹å¤±å»è¯­ä¹‰é‡å¿ƒã€‚

### âœ…å½“å‰æ ¸å¿ƒåŠŸèƒ½
æœ¬é¡¹ç›®ç›®å‰ä¸ä¾èµ–å¤æ‚çš„ç¡¬ç¼–ç è§„åˆ™ï¼Œè€Œæ˜¯é€šè¿‡ä¼˜åŒ–æ•°æ®é¢„å¤„ç†ã€æ©ç ç­–ç•¥ä¸æ‰§è¡Œæµç¨‹æ¥å¢å¼ºæ¨¡å‹èƒ½åŠ›ï¼š

**æ³¨æ„:** æœ¬é¡¹ç›®ä½¿ç”¨çš„æ˜¯ T5 è‡ªå¸¦çš„åˆ†è¯å™¨ã€‚

* **é•¿æ–‡æœ¬æŒ‰æ®µè½ä¸æ ‡ç‚¹åˆ‡åˆ†ç”Ÿæˆè®­ç»ƒæ ·æœ¬å—**  
  è‡ªåŠ¨ä»å°è¯´ã€æ–‡æ¡£æˆ–ä¸“ä¸šè¯­æ–™æ„é€ æ ·æœ¬ï¼ŒåŸºäºæ®µè½è¾¹ç•Œå’Œä¸­è‹±æ–‡æ ‡ç‚¹åˆ‡åˆ†ï¼ŒåŒæ—¶ä¿ç•™å‰æ–‡æœ«å°¾å¥ä½œä¸ºä¸Šä¸‹æ–‡å‰ç¼€ï¼Œä¿è¯æ ·æœ¬è¯­ä¹‰è¿ç»­ï¼Œç¼“è§£è·¨æ®µè¯­ä¹‰æ–­è£‚ï¼Œå¹¶å¯¹å•æ ·æœ¬é•¿åº¦è¿›è¡Œçº¦æŸä»¥é€‚é…æ¨¡å‹ä¸Šä¸‹æ–‡çª—å£ã€‚

* **ä¸“å®¶è¯åº“å¼•å¯¼çš„åŸå­åŒ–é®è”½**  
  åˆ©ç”¨è‡ªå®šä¹‰é¢†åŸŸè¯åº“ï¼Œå°†ä¸“ä¸šæœ¯è¯­ï¼ˆå¦‚ï¼šæ€¥æ€§å‰å£å¿ƒè‚Œæ¢—æ­»ã€ç»çš®å† çŠ¶åŠ¨è„‰ä»‹å…¥æ²»ç–—ï¼‰ä½œä¸ºä¸å¯åˆ†å‰²çš„æ•´ä½“è¿›è¡Œé®è”½ï¼Œé˜»æ–­å­—ç¬¦çº§æŠ•æœºè·¯å¾„ï¼Œè¿«ä½¿æ¨¡å‹ä¾èµ–ä¸Šä¸‹æ–‡è¯­ä¹‰ä¸é¢†åŸŸé€»è¾‘è¿›è¡Œè¿˜åŸã€‚

* **äººå·¥å¯æ§å¼ºåŒ–é®è”½ç­–ç•¥**  
  æ”¯æŒå¯¹é«˜éš¾åº¦æœ¯è¯­æ‰‹åŠ¨æå‡é®è”½æ¦‚ç‡ï¼ˆ**ğŸ’¡æ¨è 50%â€“70%ï¼Œä¸å®œè¶…è¿‡ 80%**ï¼‰ï¼Œå¹¶å¯åŒæ­¥è°ƒæ•´æ€»ä½“é®è”½ç‡ï¼ˆçº¦ 20%â€“25%ï¼‰ï¼Œå®ç°å®šå‘å¼ºåŒ–æ¨¡å‹åœ¨è–„å¼±çŸ¥è¯†ç‚¹ä¸Šçš„æ¨ç†èƒ½åŠ›ã€‚

* **è‡ªåŠ¨è§„é¿æ ‡ç‚¹ç¬¦å·ä¸éè¯­ä¹‰ token**  
  é®è”½è¿‡ç¨‹ä¸­è‡ªåŠ¨è·³è¿‡ä¸­è‹±æ–‡æ ‡ç‚¹ã€ç¬¦å·åŠ tokenizer æœªçŸ¥å­—ç¬¦ï¼ˆ`<unk>`ï¼‰ï¼Œä¿è¯æ¯ä¸ª mask span éƒ½å¯¹åº”æœ‰æ•ˆè¯­ä¹‰å•å…ƒã€‚

* **è¿ç»­æˆ–é‡å  span è‡ªåŠ¨åˆå¹¶**  
  å¯¹ç›¸é‚»æˆ–é‡å çš„æ©ç  span è¿›è¡Œè‡ªåŠ¨åˆå¹¶ï¼Œç¡®ä¿ç”Ÿæˆçš„ `<extra_id_n>` è¾“å…¥/è¾“å‡ºå¯¹é¡ºåºç»Ÿä¸€ã€è®­ç»ƒä¿¡å·è¿ç»­ï¼Œå®Œå…¨å…¼å®¹ T5 åŸè®ºæ–‡çš„ span corruption è®­ç»ƒè§„èŒƒã€‚

* **å¤šçº¿ç¨‹å¹¶è¡Œå¤„ç†ä¸è‡ªåŠ¨åŒ–æ ·æœ¬å›ä¼ **  
  æ”¯æŒå¯è°ƒçº¿ç¨‹æ•°å¹¶è¡Œç”Ÿæˆæ ·æœ¬ï¼Œå¤§å¹…æå‡å¤§è§„æ¨¡è¯­æ–™å¤„ç†æ•ˆç‡ï¼›æ‰€æœ‰ç”Ÿæˆæ ·æœ¬è‡ªåŠ¨å›ä¼ å¹¶æŒ‰åŸæ–‡é¡ºåºæ±‡æ€»ï¼Œæœ€ç»ˆè¾“å‡ºä¸ºå¯ç›´æ¥ç”¨äºè®­ç»ƒçš„æ ‡å‡†åŒ– JSONL æ•°æ®é›†ã€‚

**é€šè¿‡ä¸Šä¸‹æ–‡æ„ŸçŸ¥çš„æ•°æ®æ„é€ ã€å…³é”®è¯ä¼˜å…ˆé®è”½ã€éšæœº span æ©ç åŠäººä¸ºåˆ¶é€ çš„â€œæç«¯ä¿¡æ¯ç¼ºå¤±â€åœºæ™¯ï¼Œæ¨¡å‹åœ¨æœ€ä¸åˆ©è¾“å…¥æ¡ä»¶ä¸‹ä¾ç„¶èƒ½å¤Ÿä¿æŒå¯¹ä¸“ä¸šè¯­ä¹‰çš„å‡†ç¡®ç†è§£ä¸è¿˜åŸèƒ½åŠ›ã€‚**  

> é€‚ç”¨åœºæ™¯ï¼šåŒ»å­¦ã€æ³•å¾‹ã€æŠ€æœ¯æ–‡æ¡£ç­‰éœ€è¦ç²¾ç¡®è¯­ä¹‰ç†è§£çš„ä¸“ä¸šè¯­æ–™ã€‚

### â—ï¸è®­ç»ƒæ³¨æ„äº‹é¡¹
* **é˜²æ­¢æ¨¡å‹æå‰åœæ­¢**ï¼šåœ¨é¢„å¤„ç†ä¹‹åï¼ŒT5 æ¨¡å‹å¯èƒ½ä¼šå‡ºç° Loss ä¸‹é™ç¼“æ…¢æˆ–äº§ç”Ÿå±€éƒ¨æ³¢åŠ¨çš„å‡è±¡ï¼Œå¯¼è‡´ç³»ç»Ÿé”™è¯¯åœ°æå‰åœæ­¢è®­ç»ƒã€‚
* **æ”¶æ•›åˆ¤æ–­å»ºè®®**ï¼šæ¨èå¢åŠ è®­ç»ƒæ—¶é•¿ï¼Œå¹¶æ ¹æ®å¤šä¸ªé˜¶æ®µçš„ Loss æ˜¯å¦æŒç»­ç¨³å®šä¸‹é™æ¥ç»¼åˆåˆ¤æ–­æ¨¡å‹æ”¶æ•›æƒ…å†µã€‚è‹¥è®­ç»ƒæ—¶é—´ä¸è¶³ï¼Œè¿˜åŸæ•ˆæœå¯èƒ½ä¼šå¤§æ‰“æŠ˜æ‰£ã€‚

### ğŸ“Šæ•ˆæœè¯„ä¼°
æ ¹æ®åˆæ­¥æµ‹è¯•å¯¹æ¯”ï¼Œåœ¨ mT5-base æ ‡å‡†æ¨¡å‹ä¸­ï¼š
* **æ ‡å‡†æ¨¡å‹è¡¨ç°**ï¼šåœ¨ä¸“ä¸šé¢†åŸŸçš„è¯æ±‡è¿˜åŸç‡ä¼°ç®—åœ¨ 60% ä»¥ä¸‹ï¼Œå‰©ä½™ 40% çš„è¿˜åŸç»“æœé€»è¾‘æ··ä¹±ï¼Œå‡ ä¹æ— æ³•è¢«ä¸šåŠ¡æ¥å—ã€‚
* **æœ¬é¡¹ç›®æ”¹è¿›å**ï¼šä¸“ä¸šè¯æ±‡è¿˜åŸç‡ä¼°ç®—è¾¾åˆ°äº† 85%ã€‚å‰©ä¸‹çš„ 15% è¯¯å·®ä¸­ï¼Œå¤§éƒ¨åˆ†æ˜¯è¯­ä¹‰ç›¸è¿‘çš„è¯æ±‡æ›¿ä»£ï¼Œæå¤§åœ°æé«˜äº†æ–‡æœ¬çš„æ•´ä½“å¯è¯»æ€§å’Œé€»è¾‘è¿è´¯æ€§ã€‚

### âš ï¸ä½¿ç”¨é™åˆ¶
* **ä¸Šä¸‹æ–‡ç‰‡æ®µåŒ–é™åˆ¶**ï¼šç”±äºæ¨¡å‹å•æ¬¡å¤„ç†çš„æ–‡æœ¬é•¿åº¦æœ‰é™ï¼Œä¸”æ¯æ®µæ–‡æœ¬å†…æ ‡è®°ï¼ˆMaskï¼‰çš„è¯æ±‡æ•°é‡å—é™ï¼Œé•¿æ–‡æ¡£åœ¨åˆ‡åˆ†å¤„ç†æ—¶å¯èƒ½å­˜åœ¨ä¸Šä¸‹æ–‡ä¿¡æ¯æ–­è£‚çš„æƒ…å†µï¼Œå¯¼è‡´éƒ¨åˆ†è·¨æ®µè½çš„è¯­ä¹‰æ— æ³•è¢«å®Œç¾æ•æ‰ã€‚æ¨èå›ä¼ éƒ¨åˆ†ä¸Šä¸‹æ–‡å†è®­ç»ƒã€‚
* **ç®—æ³•å±€é™æ€§**ï¼šç”±äº T5 æ¨¡å‹æœ¬èº«çš„è¿˜åŸæ˜¯åŸºäºç»Ÿè®¡æ¦‚ç‡ç®—æ³•çš„ï¼Œå› æ­¤åœ¨å¤„ç†å¤æ‚çš„æ–‡æœ¬æ—¶ï¼Œä¸å¯èƒ½ä¿è¯ 100% çš„è¿˜åŸå‡†ç¡®ç‡ã€‚
* **é¢†åŸŸä¾èµ–**ï¼šè¿˜åŸæ•ˆæœé«˜åº¦ä¾èµ–äºé¢„è®¾ä¸“å®¶è¯åº“çš„è¦†ç›–é¢ä¸æ·±åº¦ã€‚

### ğŸŒŒæœªæ¥å¼€å‘è®¡åˆ’
* è‡ªåŠ¨ç¼ºæŸæ„ŸçŸ¥:
åˆ©ç”¨åˆ†è¯å™¨çš„â€œå¼‚å¸¸ç¢ç‰‡â€ä½œä¸ºéšæ€§ä¿¡å·ã€‚å½“ OCR è¯†åˆ«å‡ºç°ä¸¥é‡é”™ä½æ—¶ï¼Œæ¨¡å‹èƒ½é€šè¿‡åˆ†è¯åºåˆ—çš„å¼‚å¸¸æ³¢åŠ¨ï¼Œè‡ªåŠ¨å®šä½åˆ°è¯­ä¹‰æ–­è£‚å¤„ã€‚
* è¯­ä¹‰è‡ªåŠ¨å¯¹é½:
æ— éœ€äººå·¥æŒ‡å®šè¡”æ¥ç‚¹ï¼Œå®ç°æ¨¡å‹å¯¹ OCR æŸåæ–‡æœ¬çš„ç«¯åˆ°ç«¯ä¿®å¤ã€‚

[Demo](#Demo) 

---
<a name="japanese"></a>
## æ—¥æœ¬èªï¼ˆæ©Ÿæ¢°ç¿»è¨³ï¼‰
**T5-Refiner-DomainFocus** ã¯ã€**äº‹å‰å­¦ç¿’æ®µéš**ã®æˆ¦ç•¥çš„æœ€é©åŒ–ã‚’é€šã˜ã¦ã€ãƒ¢ãƒ‡ãƒ«ã«å›ºæœ‰ã®ã€Œæ„å‘³çš„å¼¾åŠ›æ€§ï¼ˆSemantic Resilienceï¼‰ã€ã‚’ä»˜ä¸ã™ã‚‹ã“ã¨ã‚’ç›®çš„ã¨ã—ã¦ã„ã¾ã™ã€‚ã“ã‚Œã«ã‚ˆã‚Šã€**ãƒ†ã‚­ã‚¹ãƒˆã®æ¬ æã‚’ã‚ˆã‚Šå …ç‰¢ã«å‡¦ç†ã—ã€ãƒ‰ãƒ¡ã‚¤ãƒ³å°‚é–€çŸ¥è­˜ã‚’æ³¨å…¥ã™ã‚‹ã“ã¨ãŒå¯èƒ½ã«ãªã‚Šã¾ã™ã€‚**

### ğŸ“–ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆèƒŒæ™¯
**åŒ»ç™‚ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–ã®ãƒ‡ã‚¸ã‚¿ãƒ«åŒ–**ã«ãŠã„ã¦ã€**OCRï¼ˆå…‰å­¦æ–‡å­—èªè­˜ï¼‰** ã¯ã€ç´™ã®æå‚·ã‚„å°å½±ã®é‡ãªã‚Šãªã©ã®åŸå› ã«ã‚ˆã‚Šã€æ ¸å¿ƒçš„ãªç”¨èªã«ã€Œæ–‡å­—æ¬ æã€ãŒç”Ÿã˜ã‚‹ã“ã¨ãŒã‚ˆãã‚ã‚Šã¾ã™ã€‚
å¾“æ¥ã® **T5** ã¾ãŸã¯ **mT5** ãƒ¢ãƒ‡ãƒ«ï¼ˆç·ç§°ã—ã¦T5ï¼‰ã¯ã€ã“ã‚Œã‚‰ã®æå‚·ã—ãŸãƒ†ã‚­ã‚¹ãƒˆã‚’å‡¦ç†ã™ã‚‹éš›ã«2ã¤ã®ä¸»è¦ãªå•é¡Œã‚’æŠ±ãˆã¦ã„ã¾ã™ï¼š
* **ãƒ©ãƒ³ãƒ€ãƒ ãƒã‚¹ã‚­ãƒ³ã‚°ã®é™ç•Œ**ï¼šãƒ¢ãƒ‡ãƒ«ãŒèªæ ¹ã«åŸºã¥ã„ãŸã€Œå˜èªã®æ¨æ¸¬ã€ã‚’å­¦ç¿’ã™ã‚‹ã«ã¨ã©ã¾ã‚Šã€å®Œå…¨ãªåŒ»ç™‚æ¦‚å¿µã‚’çœŸã«ç†è§£ã§ããªã„ã€‚
* **ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚ºã®ä¸ä¸€è‡´å•é¡Œ**ï¼šç”¨èªã®æ–‡å­—ãŒæ¬ è½ã™ã‚‹ã¨ã€ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ãŒãã‚Œã‚’ç„¡æ„å‘³ãªæ–­ç‰‡ã«ç´°åˆ†åŒ–ã—ã¦ã—ã¾ã„ã€ãƒ¢ãƒ‡ãƒ«ãŒæ„å‘³ã®é‡å¿ƒã‚’å¤±ã†ã€‚

### âœ…ç¾åœ¨ã®ã‚³ã‚¢æ©Ÿèƒ½
æœ¬ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã¯ã€è¤‡é›‘ãªãƒãƒ¼ãƒ‰ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ãƒ«ãƒ¼ãƒ«ã«ä¾å­˜ã›ãšã€ãƒ‡ãƒ¼ã‚¿å‰å‡¦ç†ã€ãƒã‚¹ã‚¯æˆ¦ç•¥ã€ãŠã‚ˆã³å‡¦ç†ãƒ•ãƒ­ãƒ¼ã®æœ€é©åŒ–ã«ã‚ˆã‚Šãƒ¢ãƒ‡ãƒ«ã®èƒ½åŠ›ã‚’å¼·åŒ–ã—ã¾ã™ã€‚

**æ³¨æ„:** ã“ã®ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã§ã¯ã€T5 è‡ªèº«ã®ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã‚’ä½¿ç”¨ã—ã¦ã„ã¾ã™ã€‚

* **é•·æ–‡ã‚’æ®µè½ãŠã‚ˆã³å¥èª­ç‚¹ã§åˆ†å‰²ã—ã¦å­¦ç¿’ã‚µãƒ³ãƒ—ãƒ«ã‚’ç”Ÿæˆ**  
  å°èª¬ã€æ–‡æ›¸ã€å°‚é–€ã‚³ãƒ¼ãƒ‘ã‚¹ãªã©ã‹ã‚‰è‡ªå‹•çš„ã«ã‚µãƒ³ãƒ—ãƒ«ã‚’æ§‹ç¯‰ã—ã€æ®µè½å¢ƒç•Œã¨æ—¥è‹±å¥èª­ç‚¹ã«åŸºã¥ã„ã¦åˆ†å‰²ã—ã¾ã™ã€‚ã•ã‚‰ã«å‰ã®ãƒ†ã‚­ã‚¹ãƒˆãƒ–ãƒ­ãƒƒã‚¯ã®æœ«å°¾æ–‡ã‚’ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã¨ã—ã¦ä¿æŒã™ã‚‹ã“ã¨ã§ã€ã‚µãƒ³ãƒ—ãƒ«é–“ã®æ„å‘³ã®é€£ç¶šæ€§ã‚’ç¢ºä¿ã—ã€æ®µè½ã‚’è·¨ãæ„å‘³ã®æ–­çµ¶ã‚’ç·©å’Œã—ã¾ã™ã€‚ã¾ãŸã€ãƒ¢ãƒ‡ãƒ«ã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ã«åˆã‚ã›ã¦å˜ä¸€ã‚µãƒ³ãƒ—ãƒ«ã®é•·ã•ã‚’åˆ¶ç´„ã—ã¾ã™ã€‚

* **å°‚é–€ç”¨èªè¾æ›¸ã«åŸºã¥ãåŸå­åŒ–ãƒã‚¹ã‚­ãƒ³ã‚°ï¼ˆAtomic Maskingï¼‰**  
  ã‚«ã‚¹ã‚¿ãƒ ã®å°‚é–€ç”¨èªè¾æ›¸ã‚’åˆ©ç”¨ã—ã€å°‚é–€ç”¨èªï¼ˆä¾‹ï¼šæ€¥æ€§å‰å£å¿ƒç­‹æ¢—å¡ã€çµŒçš®çš„å† å‹•è„ˆã‚¤ãƒ³ã‚¿ãƒ¼ãƒ™ãƒ³ã‚·ãƒ§ãƒ³ï¼‰ã‚’åˆ†å‰²ä¸å¯èƒ½ãªå˜ä½ã¨ã—ã¦ãƒã‚¹ã‚¯ã—ã¾ã™ã€‚ã“ã‚Œã«ã‚ˆã‚Šã€æ–‡å­—å˜ä½ã§ã®æ¨æ¸¬ã‚’é˜²ãã€ãƒ¢ãƒ‡ãƒ«ãŒã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã®æ„å‘³ã‚„é ˜åŸŸã®è«–ç†ã«åŸºã¥ã„ã¦å¾©å…ƒã™ã‚‹ã‚ˆã†ä¿ƒã—ã¾ã™ã€‚

* **äººå·¥åˆ¶å¾¡ã«ã‚ˆã‚‹å¼·åŒ–ãƒã‚¹ã‚­ãƒ³ã‚°æˆ¦ç•¥**  
  é«˜é›£åº¦ã®å°‚é–€ç”¨èªã«ã¤ã„ã¦ãƒã‚¹ã‚¯ç¢ºç‡ã‚’æ‰‹å‹•ã§ä¸Šã’ã‚‹ã“ã¨ãŒå¯èƒ½ï¼ˆ**ğŸ’¡æ¨å¥¨ 50%â€“70%ã€80%ã‚’è¶…ãˆãªã„ã“ã¨**ï¼‰ã§ã€åŒæ™‚ã«å…¨ä½“ã®ãƒã‚¹ã‚¯ç‡ï¼ˆç´„20%â€“25%ï¼‰ã‚‚èª¿æ•´ã§ãã¾ã™ã€‚ã“ã‚Œã«ã‚ˆã‚Šã€ãƒ¢ãƒ‡ãƒ«ãŒå¼±ã„çŸ¥è­˜é ˜åŸŸã§ã®æ¨è«–èƒ½åŠ›ã‚’å¼·åŒ–ã—ã¾ã™ã€‚

* **å¥èª­ç‚¹ãŠã‚ˆã³éæ„å‘³ãƒˆãƒ¼ã‚¯ãƒ³ã®è‡ªå‹•å›é¿**  
  ãƒã‚¹ã‚­ãƒ³ã‚°æ™‚ã«æ—¥è‹±ã®å¥èª­ç‚¹ã€è¨˜å·ã€ãŠã‚ˆã³ tokenizer æœªçŸ¥æ–‡å­—ï¼ˆ`<unk>`ï¼‰ã‚’è‡ªå‹•ã§ã‚¹ã‚­ãƒƒãƒ—ã—ã€ã™ã¹ã¦ã®ãƒã‚¹ã‚¯ç¯„å›²ãŒæœ‰åŠ¹ãªæ„å‘³å˜ä½ã«å¯¾å¿œã™ã‚‹ã‚ˆã†ä¿è¨¼ã—ã¾ã™ã€‚

* **é€£ç¶šã¾ãŸã¯é‡è¤‡ã™ã‚‹spanã®è‡ªå‹•çµ±åˆ**  
  éš£æ¥ã¾ãŸã¯é‡è¤‡ã™ã‚‹ãƒã‚¹ã‚¯ç¯„å›²ã‚’è‡ªå‹•ã§çµ±åˆã—ã€ç”Ÿæˆã•ã‚Œã‚‹ `<extra_id_n>` å…¥åŠ›/å‡ºåŠ›ãƒšã‚¢ã®é †åºã‚’çµ±ä¸€ã€å­¦ç¿’ä¿¡å·ã®é€£ç¶šæ€§ã‚’ç¢ºä¿ã—ã¾ã™ã€‚ã“ã‚Œã«ã‚ˆã‚Šã€T5è«–æ–‡ã«æº–æ‹ ã—ãŸ span corruption å­¦ç¿’è¦æ ¼ã«å®Œå…¨å¯¾å¿œã—ã¾ã™ã€‚

* **ãƒãƒ«ãƒã‚¹ãƒ¬ãƒƒãƒ‰ä¸¦åˆ—å‡¦ç†ã¨ã‚µãƒ³ãƒ—ãƒ«ã®è‡ªå‹•å›å**  
  ã‚¹ãƒ¬ãƒƒãƒ‰æ•°ã‚’èª¿æ•´å¯èƒ½ãªä¸¦åˆ—å‡¦ç†ã«ã‚ˆã‚Šã€å¤§è¦æ¨¡ã‚³ãƒ¼ãƒ‘ã‚¹ã®ã‚µãƒ³ãƒ—ãƒ«ç”ŸæˆåŠ¹ç‡ã‚’å¤§å¹…ã«å‘ä¸Šã•ã›ã¾ã™ã€‚ç”Ÿæˆã•ã‚ŒãŸã™ã¹ã¦ã®ã‚µãƒ³ãƒ—ãƒ«ã¯è‡ªå‹•ã§å›åã•ã‚Œã€åŸæ–‡é †ã«æ•´ç†ã•ã‚ŒãŸå¾Œã€å­¦ç¿’ã«ç›´æ¥ä½¿ç”¨å¯èƒ½ãªæ¨™æº–JSONLãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¨ã—ã¦å‡ºåŠ›ã•ã‚Œã¾ã™ã€‚

**ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆèªè­˜å‹ã®ãƒ‡ãƒ¼ã‚¿æ§‹ç¯‰ã€ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰å„ªå…ˆãƒã‚¹ã‚­ãƒ³ã‚°ã€ãƒ©ãƒ³ãƒ€ãƒ spanãƒã‚¹ã‚¯ã€ãŠã‚ˆã³äººå·¥çš„ã«ä½œã‚‰ã‚ŒãŸã€Œæ¥µç«¯ãªæƒ…å ±æ¬ æã€ã‚·ãƒŠãƒªã‚ªã«ã‚ˆã‚Šã€ãƒ¢ãƒ‡ãƒ«ã¯æœ€ã‚‚ä¸åˆ©ãªå…¥åŠ›æ¡ä»¶ä¸‹ã§ã‚‚å°‚é–€ç”¨èªã®æ­£ç¢ºãªç†è§£ã¨å¾©å…ƒèƒ½åŠ›ã‚’ä¿æŒã§ãã¾ã™ã€‚**

> é©ç”¨ä¾‹ï¼šåŒ»å­¦ã€æ³•å¾‹ã€æŠ€è¡“æ–‡æ›¸ãªã©ã€æ­£ç¢ºãªæ„å‘³ç†è§£ãŒæ±‚ã‚ã‚‰ã‚Œã‚‹å°‚é–€ã‚³ãƒ¼ãƒ‘ã‚¹ã€‚

### â—ï¸è¨“ç·´ä¸Šã®æ³¨æ„ç‚¹
* **ãƒ¢ãƒ‡ãƒ«ã®æ—©æœŸåœæ­¢ã®é˜²æ­¢**ï¼šå‰å‡¦ç†å¾Œã€T5ãƒ¢ãƒ‡ãƒ«ã¯æå¤±ï¼ˆLossï¼‰ã®ä¸‹è½ãŒç·©ã‚„ã‹ã«ãªã£ãŸã‚Šã€å±€æ‰€çš„ãªå¤‰å‹•ãŒç”Ÿã˜ãŸã‚Šã™ã‚‹ã€Œè¦‹ã‹ã‘ä¸Šã®åœæ»ã€ãŒç™ºç”Ÿã—ã€ã‚·ã‚¹ãƒ†ãƒ ãŒèª¤ã£ã¦è¨“ç·´ã‚’æ—©æœŸçµ‚äº†ã•ã›ã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚
* **åæŸåˆ¤æ–­ã®æ¨å¥¨**ï¼šè¨“ç·´æ™‚é–“ã‚’å»¶é•·ã—ã€è¤‡æ•°ã®ãƒ•ã‚§ãƒ¼ã‚ºã§æå¤±ãŒç¶™ç¶šçš„ã«å®‰å®šã—ã¦ä¸‹è½ã—ã¦ã„ã‚‹ã‹ã«åŸºã¥ã„ã¦ã€ãƒ¢ãƒ‡ãƒ«ã®åæŸã‚’ç·åˆçš„ã«åˆ¤æ–­ã™ã‚‹ã“ã¨ã‚’æ¨å¥¨ã—ã¾ã™ã€‚è¨“ç·´æ™‚é–“ãŒä¸è¶³ã™ã‚‹ã¨ã€å¾©å…ƒåŠ¹æœãŒå¤§å¹…ã«ä½ä¸‹ã™ã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚

### ğŸ“ŠåŠ¹æœè©•ä¾¡
mT5-baseæ¨™æº–ãƒ¢ãƒ‡ãƒ«ã‚’ç”¨ã„ãŸåˆæœŸãƒ†ã‚¹ãƒˆã®æ¯”è¼ƒï¼š
* **æ¨™æº–ãƒ¢ãƒ‡ãƒ«ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹**ï¼šå°‚é–€åˆ†é‡ã®èªå½™å¾©å…ƒç‡ã¯æ¨å®š60%ä»¥ä¸‹ã€‚æ®‹ã‚Šã®40%ã¯è«–ç†ãŒæ··ä¹±ã—ã¦ãŠã‚Šã€æ¥­å‹™åˆ©ç”¨ã¯ã»ã¼ä¸å¯èƒ½ã§ã™ã€‚
* **æœ¬ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã«ã‚ˆã‚‹æ”¹å–„å¾Œ**ï¼šå°‚é–€èªå½™ã®å¾©å…ƒç‡ã¯æ¨å®š85%ã«é”ã—ã¾ã—ãŸã€‚æ®‹ã‚Šã®15%ã®èª¤å·®ã®å¤§éƒ¨åˆ†ã¯æ„å‘³ã®è¿‘ã„èªå½™ã¸ã®ç½®æ›ã§ã‚ã‚Šã€ãƒ†ã‚­ã‚¹ãƒˆå…¨ä½“ã®å¯èª­æ€§ã¨è«–ç†çš„ãªä¸€è²«æ€§ãŒå¤§å¹…ã«å‘ä¸Šã—ã¾ã—ãŸã€‚

### âš ï¸ä½¿ç”¨åˆ¶é™
* **ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã®æ–­ç‰‡åŒ–ã®åˆ¶é™**ï¼šãƒ¢ãƒ‡ãƒ«ãŒä¸€åº¦ã«å‡¦ç†ã§ãã‚‹ãƒ†ã‚­ã‚¹ãƒˆé•·ã«ã¯åˆ¶é™ãŒã‚ã‚Šã€ã¾ãŸå„ãƒ†ã‚­ã‚¹ãƒˆã‚»ã‚°ãƒ¡ãƒ³ãƒˆå†…ã§ãƒã‚¹ã‚¯ï¼ˆMaskï¼‰ã•ã‚Œã‚‹èªå½™æ•°ã‚‚é™ã‚‰ã‚Œã¦ã„ã‚‹ãŸã‚ã€é•·ã„ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’åˆ†å‰²å‡¦ç†ã™ã‚‹éš›ã«æ–‡è„ˆæƒ…å ±ãŒæ–­çµ¶ã—ã€ã‚»ã‚°ãƒ¡ãƒ³ãƒˆã‚’è·¨ãæ„å‘³ã‚’å®Œç’§ã«æ‰ãˆã‚‰ã‚Œãªã„å ´åˆãŒã‚ã‚Šã¾ã™ã€‚ä¸€éƒ¨ã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’å†åº¦å«ã‚ã¦ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã™ã‚‹ã“ã¨ã‚’æ¨å¥¨ã—ã¾ã™ã€‚
* **ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã®é™ç•Œ**ï¼šT5ãƒ¢ãƒ‡ãƒ«è‡ªä½“ã®å¾©å…ƒã¯çµ±è¨ˆçš„ç¢ºç‡ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã«åŸºã¥ã„ã¦ã„ã‚‹ãŸã‚ã€è¤‡é›‘ãªãƒ†ã‚­ã‚¹ãƒˆã‚’å‡¦ç†ã™ã‚‹éš›ã«100%ã®å¾©å…ƒç²¾åº¦ã‚’ä¿è¨¼ã™ã‚‹ã“ã¨ã¯ä¸å¯èƒ½ã§ã™ã€‚
* **ãƒ‰ãƒ¡ã‚¤ãƒ³ä¾å­˜æ€§**ï¼šå¾©å…ƒåŠ¹æœã¯ã€ã‚ã‚‰ã‹ã˜ã‚è¨­å®šã•ã‚ŒãŸå°‚é–€ç”¨èªé›†ã®ç¶²ç¾…æ€§ã¨æ·±ã•ã«å¼·ãä¾å­˜ã—ã¾ã™ã€‚

### ğŸŒŒä»Šå¾Œã®é–‹ç™ºè¨ˆç”»
* è‡ªå‹•æ¬ ææ¤œçŸ¥ï¼š
ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã®ã€Œç•°å¸¸ãªæ–­ç‰‡ã€ã‚’éš ã‚ŒãŸä¿¡å·ã¨ã—ã¦åˆ©ç”¨ã—ã¾ã™ã€‚OCRèªè­˜ã«æ·±åˆ»ãªã‚ºãƒ¬ãŒç”Ÿã˜ãŸéš›ã€ãƒ¢ãƒ‡ãƒ«ãŒãƒˆãƒ¼ã‚¯ãƒ³ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã®ç•°å¸¸ãªå¤‰å‹•ã‚’é€šã˜ã¦ã€æ„å‘³ã®æ–­çµ¶ç®‡æ‰€ã‚’è‡ªå‹•çš„ã«ç‰¹å®šã§ãã‚‹ã‚ˆã†ã«ã—ã¾ã™ã€‚
* æ„å‘³ã®è‡ªå‹•ã‚¢ãƒ©ã‚¤ãƒ¡ãƒ³ãƒˆï¼š
æ‰‹å‹•ã§æ¥ç¶šç‚¹ã‚’æŒ‡å®šã™ã‚‹ã“ã¨ãªãã€OCRã§æå‚·ã—ãŸãƒ†ã‚­ã‚¹ãƒˆã‚’ãƒ¢ãƒ‡ãƒ«ãŒã‚¨ãƒ³ãƒ‰ãƒ„ãƒ¼ã‚¨ãƒ³ãƒ‰ã§ä¿®å¾©ã§ãã‚‹ã‚ˆã†ã«ã—ã¾ã™ã€‚

[Demo](#Demo)

---
<a name="deutsch"></a>
## Deutsch (Maschinelle Ãœbersetzung)
**T5-Refiner-DomainFocus** zielt darauf ab, dem Modell durch strategische Optimierung in der **Pre-Training-Phase** eine intrinsische â€semantische Resilienzâ€œ zu verleihen, damit es **Textdefekte robuster verarbeiten und Fachwissen aus spezifischen DomÃ¤nen injizieren kann.**

### ğŸ“– Projekthintergrund
Bei der **Digitalisierung medizinischer Archive** fÃ¼hrt **OCR (optische Zeichenerkennung)** aufgrund von beschÃ¤digtem Papier, StempelÃ¼berdeckungen usw. hÃ¤ufig zu â€Zeichendefektenâ€œ bei zentralen Fachbegriffen.
HerkÃ¶mmliche **T5-** oder **mT5-Modelle** (zusammenfassend T5) haben zwei Hauptprobleme bei der Verarbeitung dieser beschÃ¤digten Texte:
* **Grenzen der zufÃ¤lligen Maskierung**: Dies fÃ¼hrt dazu, dass das Modell nur lernt, WÃ¶rter basierend auf WortstÃ¤mmen zu â€ratenâ€œ, anstatt medizinische Konzepte wirklich vollstÃ¤ndig zu verstehen.
* **Tokenisierungs-Fehlausrichtung**: Wenn Buchstaben in Fachbegriffen fehlen, zerlegt der Tokenizer diese in bedeutungslose Fragmente, wodurch das Modell seinen semantischen Fokus verliert.

### âœ… Aktuelle Kernfunktionen
Dieses Projekt ist derzeit nicht auf komplexe Hardcodierungsregeln angewiesen, sondern verbessert die ModellfÃ¤higkeiten durch optimierte Datenvorverarbeitung, Maskierungsstrategien und Ablaufsteuerung:

**Hinweis:** Dieses Projekt verwendet den T5-eigenen Tokenizer.

* **Erstellung von TrainingsdatenblÃ¶cken aus langen Texten anhand von AbsÃ¤tzen und Satzzeichen**  
  Automatische Generierung von Samples aus Romanen, Dokumenten oder Fachtexten, basierend auf Absatzgrenzen und chinesischen/englischen Satzzeichen. Gleichzeitig werden die letzten SÃ¤tze des vorherigen Blocks als KontextprÃ¤fix beibehalten, um die semantische KontinuitÃ¤t der Samples sicherzustellen, semantische BrÃ¼che zwischen AbsÃ¤tzen zu reduzieren und die LÃ¤nge einzelner Samples an das Kontextfenster des Modells anzupassen.

* **Atomare Maskierung gesteuert durch Experten-Wortlisten**  
  Verwendung benutzerdefinierter Fachwortlisten, um Fachbegriffe (z.â€¯B. *akuter Vorderwandinfarkt*, *perkutane Koronarintervention*) als unteilbare Einheiten zu maskieren. Dies verhindert spekulative Charakter-basierte Vorhersagen und zwingt das Modell, die Maske anhand des Kontextes und der Fachlogik wiederherzustellen.

* **Manuell steuerbare verstÃ¤rkte Maskierungsstrategie**  
  ErmÃ¶glicht die manuelle ErhÃ¶hung der Maskierungswahrscheinlichkeit fÃ¼r schwierige Begriffe (**ğŸ’¡ empfohlen 50%â€“70%, nicht Ã¼ber 80%**) und gleichzeitig die Anpassung der Gesamtmaskierungsrate (ca. 20%â€“25%), um gezielt die FÃ¤higkeit des Modells zu stÃ¤rken, in schwierigen Wissensbereichen semantische Schlussfolgerungen zu ziehen.

* **Automatische Vermeidung von Satzzeichen und nicht-semantischen Token**  
  WÃ¤hrend der Maskierung werden automatisch Satzzeichen, Symbole und unbekannte Token des Tokenizers (`<unk>`) Ã¼bersprungen, sodass jeder maskierte Bereich einem sinnvollen semantischen Einheit entspricht.

* **Automatische ZusammenfÃ¼hrung von aufeinanderfolgenden oder Ã¼berlappenden Spans**  
  ZusammenfÃ¼hrung benachbarter oder Ã¼berlappender Maskenspans, um sicherzustellen, dass die generierten `<extra_id_n>` Eingabe-/Ausgabe-Paare in der richtigen Reihenfolge und mit kontinuierlichem Trainingssignal vorliegen, vollstÃ¤ndig kompatibel mit der Span-Corruption-Methode des T5-Originalpapiers.

* **Mehrthreading und automatisches ZurÃ¼ckschreiben der Samples**  
  UnterstÃ¼tzung einer konfigurierbaren Anzahl von Threads zur parallelen Generierung von Samples, wodurch die Verarbeitung groÃŸer Korpora erheblich beschleunigt wird. Alle erzeugten Samples werden automatisch zurÃ¼ckgefÃ¼hrt und in der ursprÃ¼nglichen Reihenfolge zusammengefÃ¼hrt, schlieÃŸlich als standardisiertes JSONL-Dataset ausgegeben, das direkt fÃ¼r das Training verwendet werden kann.

**Durch kontextbewusste Datenkonstruktion, prioritÃ¤re Maskierung von SchlÃ¼sselbegriffen, zufÃ¤llige Span-Maskierung und absichtlich erzeugte â€extreme Informationsverlusteâ€œ-Szenarien bleibt das Modell selbst unter widrigsten Eingabebedingungen in der Lage, die Fachsemantik korrekt zu verstehen und wiederherzustellen.**

> Anwendungsbereiche: Medizinische, juristische, technische Dokumente und andere Fachtexte, die ein prÃ¤zises semantisches VerstÃ¤ndnis erfordern.

### â—ï¸ Hinweise zum Training
* **Vorzeitigen Stopp des Modells verhindern**: Nach dem Preprocessing kann es bei T5-Modellen zu einer TÃ¤uschung durch langsam sinkenden Loss oder lokale Schwankungen kommen, was dazu fÃ¼hrt, dass das System das Training fÃ¤lschlicherweise vorzeitig stoppt.
* **Empfehlung zur Konvergenzbeurteilung**: Es wird empfohlen, die Trainingsdauer zu erhÃ¶hen und die Konvergenz des Modells basierend auf dem kontinuierlichen und stabilen Sinken des Loss Ã¼ber mehrere Phasen hinweg umfassend zu beurteilen. Bei unzureichender Trainingszeit kann der Wiederherstellungseffekt stark beeintrÃ¤chtigt werden.

### ğŸ“Š EffektivitÃ¤tsbewertung
Basierend auf vorlÃ¤ufigen Vergleichstests im mT5-base Standardmodell:
* **Leistung des Standardmodells**: Die Wiederherstellungsrate von Fachvokabular wird auf unter 60% geschÃ¤tzt, wobei die restlichen 40% logisch verwirrend und fÃ¼r den geschÃ¤ftlichen Einsatz kaum akzeptabel sind.
* **Nach der Verbesserung durch dieses Projekt**: Die Wiederherstellungsrate von Fachvokabular erreichte geschÃ¤tzte 85%. Von den verbleibenden 15% Fehlerquote entfÃ¤llt der GroÃŸteil auf semantisch Ã¤hnliche Wortsubstitutionen, was die allgemeine Lesbarkeit und logische KohÃ¤renz des Textes erheblich verbessert.

### âš ï¸ NutzungseinschrÃ¤nkungen
* **EinschrÃ¤nkung durch Kontext-Fragmentierung**: Da die TextlÃ¤nge pro Verarbeitungsschritt begrenzt ist und die Anzahl der maskierten WÃ¶rter pro Textsegment limitiert ist, kann es bei der Aufteilung langer Dokumente zu BrÃ¼chen in den Kontextinformationen kommen. Dies fÃ¼hrt dazu, dass einige segmentÃ¼bergreifende Semantiken nicht perfekt erfasst werden kÃ¶nnen. Es wird empfohlen, Teile des Kontextes fÃ¼r das Re-Training zurÃ¼ckzugeben.
* **Algorithmische Grenzen**: Da die Wiederherstellung des T5-Modells auf statistischen Wahrscheinlichkeitsalgorithmen basiert, kann eine 100%ige Genauigkeit bei komplexen Texten nicht garantiert werden.
* **DomÃ¤nenabhÃ¤ngigkeit**: Der Wiederherstellungseffekt hÃ¤ngt stark von der Abdeckung und Tiefe des vordefinierten Experten-Vokabulars ab.

### ğŸŒŒ ZukÃ¼nftige EntwicklungsplÃ¤ne
* Automatische Defekterkennung:
Nutzung â€anormaler Fragmenteâ€œ des Tokenizers als implizite Signale. Wenn OCR-Erkennungen schwerwiegende Fehlausrichtungen aufweisen, kann das Modell Ã¼ber abnormale Schwankungen in der Token-Sequenz semantische BrÃ¼che automatisch lokalisieren.
* Automatische semantische Ausrichtung:
End-to-End-Reparatur von OCR-beschÃ¤digten Texten durch das Modell, ohne dass manuell VerknÃ¼pfungspunkte angegeben werden mÃ¼ssen.

[Demo](#Demo)

---
<a name="francais"></a>
## FranÃ§ais (Traduction automatique)
**T5-Refiner-DomainFocus** vise Ã  doter le modÃ¨le d'une Â« rÃ©silience sÃ©mantique Â» intrinsÃ¨que grÃ¢ce Ã  l'optimisation des stratÃ©gies lors de la **phase de prÃ©-entraÃ®nement**, **lui permettant de gÃ©rer plus solidement les lacunes textuelles et d'injecter une expertise mÃ©tier.**

### ğŸ“– Contexte du projet
Lors de la **numÃ©risation d'archives mÃ©dicales**, l'**OCR (Reconnaissance Optique de CaractÃ¨res)** entraÃ®ne souvent des Â« lacunes de caractÃ¨res Â» dans les termes clÃ©s en raison de dommages sur le papier ou de l'obstruction par des tampons.
Les modÃ¨les **T5** ou **mT5** conventionnels (collectivement appelÃ©s T5) prÃ©sentent deux problÃ¨mes majeurs lors du traitement de ces textes endommagÃ©s :
* **Limites du masquage alÃ©atoire** : Le modÃ¨le apprend uniquement Ã  Â« deviner Â» les mots Ã  partir des racines, sans vÃ©ritablement comprendre les concepts mÃ©dicaux complets.
* **ProblÃ¨me de dÃ©salignement de la tokenisation** : Lorsqu'un terme perd des lettres, le tokenizer le fragmente en morceaux dÃ©nuÃ©s de sens, faisant perdre au modÃ¨le son centre de gravitÃ© sÃ©mantique.

### âœ… Fonctions clÃ©s actuelles
Ce projet ne dÃ©pend pas de rÃ¨gles codÃ©es complexes, mais renforce les capacitÃ©s du modÃ¨le grÃ¢ce Ã  l'optimisation du prÃ©traitement des donnÃ©es, des stratÃ©gies de masquage et du flux d'exÃ©cution :

**Remarque:** Ce projet utilise le tokenizer intÃ©grÃ© de T5.

* **DÃ©coupage des textes longs en Ã©chantillons d'entraÃ®nement selon les paragraphes et la ponctuation**  
  GÃ©nÃ©ration automatique d'Ã©chantillons Ã  partir de romans, documents ou corpus spÃ©cialisÃ©s, en se basant sur les limites des paragraphes et la ponctuation en chinois et en anglais. Les phrases finales du bloc prÃ©cÃ©dent sont conservÃ©es comme prÃ©fixe contextuel pour assurer la continuitÃ© sÃ©mantique et rÃ©duire les ruptures entre les segments, tout en limitant la longueur de chaque Ã©chantillon pour s'adapter Ã  la fenÃªtre de contexte du modÃ¨le.

* **Masquage atomique guidÃ© par un lexique d'experts**  
  Utilisation d'un lexique de domaine personnalisÃ© pour traiter les termes spÃ©cialisÃ©s (ex : infarctus aigu du myocarde de la paroi antÃ©rieure, intervention coronaire percutanÃ©e) comme des unitÃ©s indivisibles lors du masquage. Cela bloque les chemins de prÃ©diction basÃ©s sur les caractÃ¨res et force le modÃ¨le Ã  s'appuyer sur le contexte et la logique du domaine pour restaurer le contenu.

* **StratÃ©gie de masquage renforcÃ©e et contrÃ´lable manuellement**  
  PossibilitÃ© d'augmenter manuellement la probabilitÃ© de masquage pour des termes difficiles (**ğŸ’¡ recommandÃ© 50%â€“70%, ne pas dÃ©passer 80%**), tout en ajustant simultanÃ©ment le taux de masquage global (environ 20%â€“25%), afin de renforcer de maniÃ¨re ciblÃ©e la capacitÃ© du modÃ¨le Ã  raisonner sur des points de connaissance faibles.

* **Ã‰vitement automatique de la ponctuation et des tokens non sÃ©mantiques**  
  Le masquage ignore automatiquement la ponctuation chinoise et anglaise, les symboles ainsi que les caractÃ¨res inconnus du tokenizer (`<unk>`), garantissant que chaque span masquÃ© correspond Ã  une unitÃ© sÃ©mantique valide.

* **Fusion automatique des spans consÃ©cutifs ou chevauchants**  
  Les spans de masquage adjacents ou chevauchants sont fusionnÃ©s automatiquement, assurant un ordre cohÃ©rent des entrÃ©es/sorties `<extra_id_n>` et un signal d'entraÃ®nement continu, entiÃ¨rement compatible avec la norme de span corruption du papier original T5.

* **Traitement parallÃ¨le multithread et retour automatisÃ© des Ã©chantillons**  
  GÃ©nÃ©ration d'Ã©chantillons en parallÃ¨le avec un nombre de threads configurable, amÃ©liorant considÃ©rablement l'efficacitÃ© sur de grands corpus ; tous les Ã©chantillons gÃ©nÃ©rÃ©s sont automatiquement rÃ©intÃ©grÃ©s et triÃ©s selon l'ordre original du texte, pour produire un dataset JSONL standard directement exploitable pour l'entraÃ®nement.

**GrÃ¢ce Ã  la construction de donnÃ©es consciente du contexte, au masquage prioritaire des mots-clÃ©s, au masquage alÃ©atoire de spans et Ã  la crÃ©ation de scÃ©narios de "perte d'information extrÃªme", le modÃ¨le reste capable de comprendre et de restaurer avec prÃ©cision le sens des termes spÃ©cialisÃ©s mÃªme dans des conditions d'entrÃ©e les plus dÃ©favorables.**

> Cas d'utilisation : corpus spÃ©cialisÃ©s nÃ©cessitant une comprÃ©hension sÃ©mantique prÃ©cise, tels que la mÃ©decine, le droit ou les documents techniques.

### â—ï¸ PrÃ©cautions d'entraÃ®nement
* **PrÃ©venir l'arrÃªt prÃ©maturÃ© du modÃ¨le** : AprÃ¨s le prÃ©traitement, le modÃ¨le T5 peut donner l'illusion d'une baisse lente de la perte (Loss) ou de fluctuations locales, ce qui peut amener le systÃ¨me Ã  arrÃªter l'entraÃ®nement prÃ©maturÃ©ment par erreur.
* **Conseils pour juger de la convergence** : Il est recommandÃ© d'augmenter la durÃ©e d'entraÃ®nement et de juger de la convergence de maniÃ¨re globale en vÃ©rifiant si la perte continue de descendre de faÃ§on stable sur plusieurs Ã©tapes. Si le temps d'entraÃ®nement est insuffisant, l'effet de restauration pourrait Ãªtre considÃ©rablement rÃ©duit.

### ğŸ“Š Ã‰valuation des rÃ©sultats
Selon les tests comparatifs prÃ©liminaires sur le modÃ¨le standard mT5-base :
* **Performance du modÃ¨le standard** : Le taux de restauration du vocabulaire spÃ©cialisÃ© est estimÃ© Ã  moins de 60 %, les 40 % restants Ã©tant logiquement confus et pratiquement inacceptables pour une utilisation mÃ©tier.
* **AprÃ¨s amÃ©lioration par ce projet** : Le taux de restauration du vocabulaire spÃ©cialisÃ© atteint environ 85 %. Parmi les 15 % d'erreurs restantes, la plupart sont des substitutions par des termes sÃ©mantiquement proches, ce qui amÃ©liore considÃ©rablement la lisibilitÃ© globale et la cohÃ©rence logique du texte.

### âš ï¸ Limites d'utilisation
* **Limitation de la fragmentation du contexte** : En raison de la longueur limitÃ©e du texte traitÃ© en une seule fois et du nombre restreint de mots masquÃ©s (Mask) par segment, le traitement de documents longs peut entraÃ®ner une rupture des informations contextuelles, empÃªchant la capture parfaite de la sÃ©mantique entre les paragraphes. Il est recommandÃ© de rÃ©injecter une partie du contexte pour le rÃ©entraÃ®nement.
* **Limites algorithmiques** : La restauration du modÃ¨le T5 Ã©tant basÃ©e sur des algorithmes de probabilitÃ© statistique, il est impossible de garantir une prÃ©cision de restauration de 100 % lors du traitement de textes complexes.
* **DÃ©pendance au domaine** : L'efficacitÃ© de la restauration dÃ©pend fortement de la couverture et de la profondeur du lexique d'experts prÃ©dÃ©fini.

### ğŸŒŒ Plan de dÃ©veloppement futur
* Perception automatique des lacunes :
Utiliser les Â« fragments anormaux Â» du tokenizer comme signaux implicites. En cas de dÃ©calage grave de l'OCR, le modÃ¨le pourra localiser automatiquement les ruptures sÃ©mantiques via les fluctuations anormales de la sÃ©quence de tokens.
* Alignement sÃ©mantique automatique :
RÃ©aliser une rÃ©paration de bout en bout des textes endommagÃ©s par l'OCR sans avoir besoin de spÃ©cifier manuellement les points de jonction.

[Demo](#Demo)

---
<a name="espanol"></a>
## EspaÃ±ol (TraducciÃ³n AutomÃ¡tica)
**T5-Refiner-DomainFocus** estÃ¡ diseÃ±ado para dotar al modelo de una "resiliencia semÃ¡ntica" intrÃ­nseca mediante la optimizaciÃ³n estratÃ©gica de la **fase de preentrenamiento**, **permitiÃ©ndole manejar de manera mÃ¡s robusta la pÃ©rdida de texto e inyectar conocimientos especializados del dominio.**

### ğŸ“– Antecedentes del Proyecto
Al procesar la **digitalizaciÃ³n de archivos mÃ©dicos**, el **OCR (Reconocimiento Ã“ptico de Caracteres)** suele presentar "defectos de caracteres" en tÃ©rminos clave debido a daÃ±os en el papel, obstrucciÃ³n por sellos, entre otros motivos.
Los modelos tradicionales **T5** o **mT5** (colectivamente T5) presentan dos problemas principales al manejar estos textos daÃ±ados:
* **Limitaciones del enmascaramiento aleatorio**: El modelo solo aprende a "adivinar palabras" basÃ¡ndose en raÃ­ces lÃ©xicas, sin comprender realmente los conceptos mÃ©dicos completos.
* **Problemas de desalineaciÃ³n de la tokenizaciÃ³n**: Cuando un tÃ©rmino pierde letras, el tokenizador lo fragmenta en pedazos sin sentido, lo que hace que el modelo pierda el centro de gravedad semÃ¡ntico.

### âœ… Funciones Principales Actuales
Este proyecto actualmente no depende de reglas codificadas complejas, sino que mejora la capacidad del modelo a travÃ©s de la optimizaciÃ³n del preprocesamiento de datos, las estrategias de enmascaramiento y el flujo de ejecuciÃ³n:

**Nota:** Este proyecto utiliza el tokenizador propio de T5.

* **GeneraciÃ³n de bloques de entrenamiento a partir de textos largos mediante segmentaciÃ³n por pÃ¡rrafos y puntuaciÃ³n**  
  Construye automÃ¡ticamente muestras a partir de novelas, documentos o corpus especializados, segmentando segÃºn los lÃ­mites de pÃ¡rrafos y la puntuaciÃ³n en chino e inglÃ©s. AdemÃ¡s, conserva la Ãºltima oraciÃ³n del bloque anterior como prefijo contextual, asegurando la continuidad semÃ¡ntica de las muestras, mitigando rupturas entre pÃ¡rrafos y ajustando la longitud de cada muestra para adaptarse a la ventana de contexto del modelo.

* **Enmascaramiento atÃ³mico guiado por un vocabulario de expertos**  
  Aprovecha un vocabulario de dominio personalizado para tratar tÃ©rminos especializados (por ejemplo: infarto agudo de pared anterior, intervenciÃ³n coronaria percutÃ¡nea) como unidades indivisibles al enmascarar, bloqueando rutas de predicciÃ³n a nivel de carÃ¡cter y obligando al modelo a restaurar la informaciÃ³n basÃ¡ndose en la semÃ¡ntica del contexto y la lÃ³gica del dominio.

* **Estrategia de enmascaramiento reforzada y controlable manualmente**  
  Permite aumentar manualmente la probabilidad de enmascaramiento de tÃ©rminos complejos (**ğŸ’¡ recomendado 50%â€“70%, no superar 80%**) y ajustar simultÃ¡neamente la tasa de enmascaramiento global (aprox. 20%â€“25%), para reforzar de manera dirigida la capacidad de razonamiento del modelo sobre puntos de conocimiento dÃ©biles.

* **Evitar automÃ¡ticamente puntuaciÃ³n y tokens no semÃ¡nticos**  
  Durante el enmascaramiento, se omiten automÃ¡ticamente signos de puntuaciÃ³n en chino e inglÃ©s, sÃ­mbolos y caracteres desconocidos para el tokenizer (`<unk>`), asegurando que cada span enmascarado corresponda a una unidad semÃ¡ntica vÃ¡lida.

* **FusiÃ³n automÃ¡tica de spans consecutivos o superpuestos**  
  Los spans de enmascaramiento adyacentes o superpuestos se fusionan automÃ¡ticamente, garantizando que los pares de entrada/salida `<extra_id_n>` se generen en orden, con seÃ±ales de entrenamiento continuas, cumpliendo totalmente con la especificaciÃ³n de span corruption de T5.

* **Procesamiento paralelo multihilo y retorno automÃ¡tico de muestras**  
  Soporta generaciÃ³n paralela de muestras con un nÃºmero configurable de hilos, aumentando significativamente la eficiencia en corpus de gran escala; todas las muestras generadas se devuelven automÃ¡ticamente y se reorganizan segÃºn el orden original, produciendo finalmente un conjunto de datos JSONL estandarizado listo para entrenamiento.

**Mediante la construcciÃ³n de datos consciente del contexto, enmascaramiento prioritario de palabras clave, spans aleatorios y escenarios de â€œpÃ©rdida extrema de informaciÃ³nâ€ artificialmente creados, el modelo puede mantener la comprensiÃ³n y restauraciÃ³n precisa de la semÃ¡ntica profesional incluso en las condiciones de entrada mÃ¡s desfavorables.**

> Escenarios de aplicaciÃ³n: corpus especializados en medicina, derecho, documentos tÃ©cnicos u otros que requieran comprensiÃ³n semÃ¡ntica precisa.

### â—ï¸ Notas sobre el Entrenamiento
* **Prevenir la parada temprana del modelo**: Tras el preprocesamiento, el modelo T5 puede mostrar una caÃ­da lenta de la pÃ©rdida (Loss) o fluctuaciones locales, lo que podrÃ­a llevar al sistema a detener el entrenamiento prematuramente por error.
* **Sugerencia para juzgar la convergencia**: Se recomienda aumentar la duraciÃ³n del entrenamiento y juzgar la convergencia del modelo basÃ¡ndose en si la pÃ©rdida disminuye de forma estable y continua a travÃ©s de mÃºltiples etapas. Si el tiempo de entrenamiento es insuficiente, el efecto de restauraciÃ³n podrÃ­a verse seriamente afectado.

### ğŸ“Š EvaluaciÃ³n de Resultados
SegÃºn las comparaciones de pruebas preliminares en el modelo estÃ¡ndar mT5-base:
* **Rendimiento del modelo estÃ¡ndar**: La tasa de restauraciÃ³n de vocabulario profesional se estima por debajo del 60%, y el 40% restante de los resultados carece de coherencia lÃ³gica, siendo casi inaceptable para el uso empresarial.
* **Tras las mejoras de este proyecto**: La tasa de restauraciÃ³n de vocabulario profesional estimada alcanzÃ³ el 85%. En el 15% de error restante, la mayorÃ­a son sustituciones por sinÃ³nimos cercanos, lo que mejora enormemente la legibilidad general y la coherencia lÃ³gica del texto.

### âš ï¸ Limitaciones de Uso
* **LimitaciÃ³n por fragmentaciÃ³n de contexto**: Debido a que la longitud de texto que el modelo procesa por vez es limitada, y el nÃºmero de palabras enmascaradas (Mask) dentro de cada fragmento tambiÃ©n lo estÃ¡, los documentos largos pueden sufrir rupturas de informaciÃ³n contextual al ser segmentados, lo que impide capturar perfectamente la semÃ¡ntica entre pÃ¡rrafos. Se recomienda reintroducir parte del contexto para el reentrenamiento.
* **Limitaciones del algoritmo**: Dado que la restauraciÃ³n del modelo T5 se basa en algoritmos de probabilidad estadÃ­stica, es imposible garantizar una precisiÃ³n de restauraciÃ³n del 100% al tratar textos complejos.
* **Dependencia del dominio**: El efecto de restauraciÃ³n depende altamente de la cobertura y profundidad del lÃ©xico experto preestablecido.

### ğŸŒŒ Plan de Desarrollo Futuro
* PercepciÃ³n AutomÃ¡tica de Defectos:
Utilizar los "fragmentos anÃ³malos" del tokenizador como seÃ±ales implÃ­citas. Cuando el OCR presenta una desalineaciÃ³n grave, el modelo podrÃ¡ localizar automÃ¡ticamente la ruptura semÃ¡ntica a travÃ©s de las fluctuaciones anormales en la secuencia de tokens.
* AlineaciÃ³n SemÃ¡ntica AutomÃ¡tica:
Lograr la reparaciÃ³n de extremo a extremo del texto daÃ±ado por OCR sin necesidad de especificar manualmente los puntos de uniÃ³n.

[Demo](#Demo)

---
<a name="hindi"></a>
## à¤¹à¤¿à¤¨à¥à¤¦à¥€ï¼ˆà¤®à¤¶à¥€à¤¨ à¤…à¤¨à¥à¤µà¤¾à¤¦ï¼‰
**T5-Refiner-DomainFocus** à¤•à¤¾ à¤‰à¤¦à¥à¤¦à¥‡à¤¶à¥à¤¯ **à¤ªà¥‚à¤°à¥à¤µ-à¤ªà¥à¤°à¤¶à¤¿à¤•à¥à¤·à¤£ à¤šà¤°à¤£ (pre-training stage)** à¤•à¥‡ à¤°à¤£à¤¨à¥€à¤¤à¤¿à¤• à¤…à¤¨à¥à¤•à¥‚à¤²à¤¨ à¤•à¥‡ à¤®à¤¾à¤§à¥à¤¯à¤® à¤¸à¥‡ à¤®à¥‰à¤¡à¤² à¤•à¥‹ à¤à¤• à¤†à¤‚à¤¤à¤°à¤¿à¤• "à¤¸à¤¿à¤®à¥‡à¤‚à¤Ÿà¤¿à¤• à¤²à¤šà¥€à¤²à¤¾à¤ªà¤¨ (Semantic Resilience)" à¤ªà¥à¤°à¤¦à¤¾à¤¨ à¤•à¤°à¤¨à¤¾ à¤¹à¥ˆ, **à¤¤à¤¾à¤•à¤¿ à¤¯à¤¹ à¤Ÿà¥‡à¤•à¥à¤¸à¥à¤Ÿ à¤•à¥€ à¤•à¤®à¥€ à¤•à¥‹ à¤…à¤§à¤¿à¤• à¤®à¤œà¤¬à¥‚à¤¤à¥€ à¤¸à¥‡ à¤¸à¤‚à¤­à¤¾à¤² à¤¸à¤•à¥‡ à¤”à¤° à¤¡à¥‹à¤®à¥‡à¤¨ à¤µà¤¿à¤¶à¥‡à¤·à¤œà¥à¤à¤¤à¤¾ à¤•à¥‹ à¤¸à¤®à¤¾à¤¹à¤¿à¤¤ à¤•à¤° à¤¸à¤•à¥‡à¥¤**

### ğŸ“–à¤ªà¥à¤°à¥‹à¤œà¥‡à¤•à¥à¤Ÿ à¤•à¥€ à¤ªà¥ƒà¤·à¥à¤ à¤­à¥‚à¤®à¤¿
**à¤šà¤¿à¤•à¤¿à¤¤à¥à¤¸à¤¾ à¤…à¤­à¤¿à¤²à¥‡à¤–à¤¾à¤—à¤¾à¤° (medical archives) à¤•à¥‡ à¤¡à¤¿à¤œà¤¿à¤Ÿà¤²à¥€à¤•à¤°à¤£** à¤•à¥‡ à¤¦à¥Œà¤°à¤¾à¤¨, **OCR (à¤‘à¤ªà¥à¤Ÿà¤¿à¤•à¤² à¤•à¥ˆà¤°à¥‡à¤•à¥à¤Ÿà¤° à¤°à¤¿à¤•à¤—à¥à¤¨à¤¿à¤¶à¤¨)** à¤…à¤•à¥à¤¸à¤° à¤•à¤¾à¤—à¤œ à¤•à¥€ à¤•à¥à¤·à¤¤à¤¿ à¤¯à¤¾ à¤¸à¥à¤Ÿà¥ˆà¤®à¥à¤ª à¤•à¥‡ à¤…à¤µà¤°à¥‹à¤§ à¤œà¥ˆà¤¸à¥‡ à¤•à¤¾à¤°à¤£à¥‹à¤‚ à¤¸à¥‡ à¤®à¥à¤–à¥à¤¯ à¤¶à¤¬à¥à¤¦à¤¾à¤µà¤²à¥€ à¤®à¥‡à¤‚ "à¤•à¥ˆà¤°à¥‡à¤•à¥à¤Ÿà¤° à¤•à¥€ à¤•à¤®à¥€" à¤ªà¥ˆà¤¦à¤¾ à¤•à¤° à¤¦à¥‡à¤¤à¤¾ à¤¹à¥ˆà¥¤
à¤ªà¤¾à¤°à¤‚à¤ªà¤°à¤¿à¤• **T5** à¤¯à¤¾ **mT5** à¤®à¥‰à¤¡à¤² (à¤¸à¤¾à¤®à¥‚à¤¹à¤¿à¤• à¤°à¥‚à¤ª à¤¸à¥‡ T5) à¤‡à¤¨ à¤•à¥à¤·à¤¤à¤¿à¤—à¥à¤°à¤¸à¥à¤¤ à¤Ÿà¥‡à¤•à¥à¤¸à¥à¤Ÿ à¤•à¥‹ à¤¸à¤‚à¤­à¤¾à¤²à¤¤à¥‡ à¤¸à¤®à¤¯ à¤¦à¥‹ à¤®à¥à¤–à¥à¤¯ à¤¸à¤®à¤¸à¥à¤¯à¤¾à¤“à¤‚ à¤•à¤¾ à¤¸à¤¾à¤®à¤¨à¤¾ à¤•à¤°à¤¤à¥‡ à¤¹à¥ˆà¤‚:
* **à¤¯à¤¾à¤¦à¥ƒà¤šà¥à¤›à¤¿à¤• à¤®à¤¾à¤¸à¥à¤•à¤¿à¤‚à¤— (Random Masking) à¤•à¥€ à¤¸à¥€à¤®à¤¾à¤à¤**: à¤®à¥‰à¤¡à¤² à¤•à¥‡à¤µà¤² à¤¶à¤¬à¥à¤¦ à¤•à¥€ à¤œà¤¡à¤¼ à¤•à¥‡ à¤†à¤§à¤¾à¤° à¤ªà¤° "à¤¶à¤¬à¥à¤¦ à¤•à¤¾ à¤…à¤¨à¥à¤®à¤¾à¤¨ à¤²à¤—à¤¾à¤¨à¤¾" à¤¸à¥€à¤–à¤¤à¤¾ à¤¹à¥ˆ, à¤œà¤¬à¤•à¤¿ à¤µà¤¹ à¤ªà¥‚à¤°à¥à¤£ à¤šà¤¿à¤•à¤¿à¤¤à¥à¤¸à¤¾ à¤…à¤µà¤§à¤¾à¤°à¤£à¤¾à¤“à¤‚ à¤•à¥‹ à¤µà¤¾à¤¸à¥à¤¤à¤µ à¤®à¥‡à¤‚ à¤¨à¤¹à¥€à¤‚ à¤¸à¤®à¤ à¤ªà¤¾à¤¤à¤¾à¥¤
* **à¤Ÿà¥‹à¤•à¤¨à¤¾à¤‡à¤œà¤¼à¥‡à¤¶à¤¨ à¤®à¤¿à¤¸à¤…à¤²à¤¾à¤‡à¤¨à¤®à¥‡à¤‚à¤Ÿ à¤•à¥€ à¤¸à¤®à¤¸à¥à¤¯à¤¾**: à¤œà¤¬ à¤¶à¤¬à¥à¤¦à¤¾à¤µà¤²à¥€ à¤•à¥‡ à¤…à¤•à¥à¤·à¤° à¤—à¤¾à¤¯à¤¬ à¤¹à¥‹à¤¤à¥‡ à¤¹à¥ˆà¤‚, à¤¤à¥‹ à¤Ÿà¥‹à¤•à¤¨à¤¾à¤‡à¤œà¤¼à¤° à¤‰à¤¸à¥‡ à¤…à¤°à¥à¤¥à¤¹à¥€à¤¨ à¤Ÿà¥à¤•à¤¡à¤¼à¥‹à¤‚ à¤®à¥‡à¤‚ à¤•à¤¾à¤Ÿ à¤¦à¥‡à¤¤à¤¾ à¤¹à¥ˆ, à¤œà¤¿à¤¸à¤¸à¥‡ à¤®à¥‰à¤¡à¤² à¤…à¤ªà¤¨à¤¾ à¤¸à¤¿à¤®à¥‡à¤‚à¤Ÿà¤¿à¤• à¤•à¥‡à¤‚à¤¦à¥à¤° à¤–à¥‹ à¤¦à¥‡à¤¤à¤¾ à¤¹à¥ˆà¥¤

### âœ…à¤µà¤°à¥à¤¤à¤®à¤¾à¤¨ à¤®à¥à¤–à¥à¤¯ à¤µà¤¿à¤¶à¥‡à¤·à¤¤à¤¾à¤à¤‚
à¤‡à¤¸ à¤ªà¤°à¤¿à¤¯à¥‹à¤œà¤¨à¤¾ à¤®à¥‡à¤‚ à¤µà¤°à¥à¤¤à¤®à¤¾à¤¨ à¤®à¥‡à¤‚ à¤œà¤Ÿà¤¿à¤² à¤¹à¤¾à¤°à¥à¤¡à¤•à¥‹à¤¡à¥‡à¤¡ à¤¨à¤¿à¤¯à¤®à¥‹à¤‚ à¤ªà¤° à¤¨à¤¿à¤°à¥à¤­à¤°à¤¤à¤¾ à¤¨à¤¹à¥€à¤‚ à¤¹à¥ˆ, à¤¬à¤²à¥à¤•à¤¿ à¤¡à¥‡à¤Ÿà¤¾ à¤ªà¥‚à¤°à¥à¤µ-à¤¸à¤‚à¤¸à¤¾à¤§à¤¨, à¤®à¤¾à¤¸à¥à¤•à¤¿à¤‚à¤— à¤°à¤£à¤¨à¥€à¤¤à¤¿à¤¯à¥‹à¤‚ à¤”à¤° à¤¨à¤¿à¤·à¥à¤ªà¤¾à¤¦à¤¨ à¤ªà¥à¤°à¤µà¤¾à¤¹ à¤•à¥‹ à¤…à¤¨à¥à¤•à¥‚à¤²à¤¿à¤¤ à¤•à¤°à¤•à¥‡ à¤®à¥‰à¤¡à¤² à¤•à¥€ à¤•à¥à¤·à¤®à¤¤à¤¾ à¤¬à¤¢à¤¼à¤¾à¤ˆ à¤œà¤¾à¤¤à¥€ à¤¹à¥ˆ:

**à¤§à¥à¤¯à¤¾à¤¨ à¤¦à¥‡à¤‚:** à¤‡à¤¸ à¤ªà¥à¤°à¥‹à¤œà¥‡à¤•à¥à¤Ÿ à¤®à¥‡à¤‚ T5 à¤•à¤¾ à¤…à¤ªà¤¨à¤¾ à¤Ÿà¥‹à¤•à¤¨à¤¾à¤‡à¤œà¤¼à¤° à¤‰à¤ªà¤¯à¥‹à¤— à¤•à¤¿à¤¯à¤¾ à¤—à¤¯à¤¾ à¤¹à¥ˆà¥¤

* **à¤²à¤‚à¤¬à¥‡ à¤Ÿà¥‡à¤•à¥à¤¸à¥à¤Ÿ à¤•à¥‹ à¤ªà¥ˆà¤°à¤¾ à¤”à¤° à¤µà¤¿à¤°à¤¾à¤® à¤šà¤¿à¤¹à¥à¤¨ à¤•à¥‡ à¤…à¤¨à¥à¤¸à¤¾à¤° à¤ªà¥à¤°à¤¶à¤¿à¤•à¥à¤·à¤£ à¤¨à¤®à¥‚à¤¨à¤¾ à¤¬à¥à¤²à¥‰à¤• à¤®à¥‡à¤‚ à¤µà¤¿à¤­à¤¾à¤œà¤¿à¤¤ à¤•à¤°à¤¨à¤¾**  
  à¤¸à¥à¤µà¤šà¤¾à¤²à¤¿à¤¤ à¤°à¥‚à¤ª à¤¸à¥‡ à¤‰à¤ªà¤¨à¥à¤¯à¤¾à¤¸, à¤¦à¤¸à¥à¤¤à¤¾à¤µà¥‡à¤œà¤¼ à¤¯à¤¾ à¤ªà¥‡à¤¶à¥‡à¤µà¤° à¤•à¥‰à¤°à¥à¤ªà¤¸ à¤¸à¥‡ à¤¨à¤®à¥‚à¤¨à¥‡ à¤¤à¥ˆà¤¯à¤¾à¤° à¤•à¤¿à¤ à¤œà¤¾à¤¤à¥‡ à¤¹à¥ˆà¤‚, à¤ªà¥ˆà¤°à¤¾ à¤•à¥€ à¤¸à¥€à¤®à¤¾à¤“à¤‚ à¤”à¤° à¤…à¤‚à¤—à¥à¤°à¥‡à¤œà¤¼à¥€-à¤šà¥€à¤¨à¥€ à¤µà¤¿à¤°à¤¾à¤® à¤šà¤¿à¤¹à¥à¤¨à¥‹à¤‚ à¤•à¥‡ à¤†à¤§à¤¾à¤° à¤ªà¤° à¤µà¤¿à¤­à¤¾à¤œà¤¨ à¤•à¤¿à¤¯à¤¾ à¤œà¤¾à¤¤à¤¾ à¤¹à¥ˆ, à¤¸à¤¾à¤¥ à¤¹à¥€ à¤ªà¤¿à¤›à¤²à¥‡ à¤Ÿà¥‡à¤•à¥à¤¸à¥à¤Ÿ à¤•à¥€ à¤…à¤‚à¤¤à¤¿à¤® à¤ªà¤‚à¤•à¥à¤¤à¤¿ à¤•à¥‹ à¤¸à¤‚à¤¦à¤°à¥à¤­ à¤ªà¥‚à¤°à¥à¤µà¤µà¤°à¥à¤¤à¥€ (context prefix) à¤•à¥‡ à¤°à¥‚à¤ª à¤®à¥‡à¤‚ à¤°à¤–à¤¾ à¤œà¤¾à¤¤à¤¾ à¤¹à¥ˆ à¤¤à¤¾à¤•à¤¿ à¤¨à¤®à¥‚à¤¨à¥‡ à¤•à¥€ à¤…à¤°à¥à¤¥à¤ªà¥‚à¤°à¥à¤£ à¤¨à¤¿à¤°à¤‚à¤¤à¤°à¤¤à¤¾ à¤¬à¤¨à¥€ à¤°à¤¹à¥‡, à¤…à¤‚à¤¶à¥‹à¤‚ à¤•à¥‡ à¤¬à¥€à¤š à¤…à¤°à¥à¤¥ à¤¸à¤‚à¤¬à¤‚à¤§ à¤Ÿà¥‚à¤Ÿà¤¨à¥‡ à¤•à¥€ à¤¸à¤‚à¤­à¤¾à¤µà¤¨à¤¾ à¤•à¤® à¤¹à¥‹ à¤”à¤° à¤ªà¥à¤°à¤¤à¥à¤¯à¥‡à¤• à¤¨à¤®à¥‚à¤¨à¥‡ à¤•à¥€ à¤²à¤‚à¤¬à¤¾à¤ˆ à¤®à¥‰à¤¡à¤² à¤•à¥‡ à¤¸à¤‚à¤¦à¤°à¥à¤­ à¤µà¤¿à¤‚à¤¡à¥‹ à¤•à¥‡ à¤…à¤¨à¥à¤°à¥‚à¤ª à¤¨à¤¿à¤¯à¤‚à¤¤à¥à¤°à¤¿à¤¤ à¤°à¤¹à¥‡à¥¤

* **à¤µà¤¿à¤¶à¥‡à¤·à¤œà¥à¤ à¤¶à¤¬à¥à¤¦à¤•à¥‹à¤¶ à¤¦à¥à¤µà¤¾à¤°à¤¾ à¤¨à¤¿à¤°à¥à¤¦à¥‡à¤¶à¤¿à¤¤ à¤ªà¤°à¤®à¤¾à¤£à¥ à¤®à¤¾à¤¸à¥à¤•à¤¿à¤‚à¤— (Atomic Masking)**  
  à¤•à¤¸à¥à¤Ÿà¤® à¤¡à¥‹à¤®à¥‡à¤¨ à¤¶à¤¬à¥à¤¦à¤•à¥‹à¤¶ à¤•à¤¾ à¤‰à¤ªà¤¯à¥‹à¤— à¤•à¤°à¤•à¥‡ à¤ªà¥‡à¤¶à¥‡à¤µà¤° à¤¶à¤¬à¥à¤¦à¥‹à¤‚ (à¤œà¥ˆà¤¸à¥‡: à¤¤à¥€à¤µà¥à¤° à¤…à¤—à¥à¤°à¤­à¤¾à¤— à¤¹à¥ƒà¤¦à¤¯à¤¾à¤˜à¤¾à¤¤, à¤ªà¤°à¥à¤•à¥à¤Ÿà¥‡à¤¨à¤¿à¤¯à¤¸ à¤•à¥‹à¤°à¥‹à¤¨à¤°à¥€ à¤‡à¤‚à¤Ÿà¤°à¤µà¥‡à¤‚à¤¶à¤¨) à¤•à¥‹ à¤…à¤ªà¤°à¤¿à¤­à¤¾à¤œà¥à¤¯ à¤‡à¤•à¤¾à¤ˆ à¤•à¥‡ à¤°à¥‚à¤ª à¤®à¥‡à¤‚ à¤®à¤¾à¤¸à¥à¤• à¤•à¤¿à¤¯à¤¾ à¤œà¤¾à¤¤à¤¾ à¤¹à¥ˆ, à¤œà¤¿à¤¸à¤¸à¥‡ à¤…à¤•à¥à¤·à¤°-à¤¸à¥à¤¤à¤°à¥€à¤¯ à¤…à¤¨à¥à¤®à¤¾à¤¨ à¤²à¤—à¤¾à¤¨à¥‡ à¤•à¥‡ à¤°à¤¾à¤¸à¥à¤¤à¥‡ à¤…à¤µà¤°à¥à¤¦à¥à¤§ à¤¹à¥‹à¤¤à¥‡ à¤¹à¥ˆà¤‚ à¤”à¤° à¤®à¥‰à¤¡à¤² à¤•à¥‹ à¤•à¥‡à¤µà¤² à¤¸à¤‚à¤¦à¤°à¥à¤­ à¤…à¤°à¥à¤¥ à¤”à¤° à¤¡à¥‹à¤®à¥‡à¤¨ à¤²à¥‰à¤œà¤¿à¤• à¤ªà¤° à¤­à¤°à¥‹à¤¸à¤¾ à¤•à¤°à¤•à¥‡ à¤ªà¥à¤¨à¤°à¥à¤¨à¤¿à¤°à¥à¤®à¤¾à¤£ à¤•à¤°à¤¨à¤¾ à¤ªà¤¡à¤¼à¤¤à¤¾ à¤¹à¥ˆà¥¤

* **à¤®à¤¾à¤¨à¤µ-à¤¨à¤¿à¤¯à¤‚à¤¤à¥à¤°à¤¿à¤¤ à¤¸à¤¶à¤•à¥à¤¤ à¤®à¤¾à¤¸à¥à¤•à¤¿à¤‚à¤— à¤°à¤£à¤¨à¥€à¤¤à¤¿**  
  à¤‰à¤šà¥à¤š à¤•à¤ à¤¿à¤¨à¤¾à¤ˆ à¤µà¤¾à¤²à¥‡ à¤¶à¤¬à¥à¤¦à¥‹à¤‚ à¤•à¥‡ à¤²à¤¿à¤ à¤®à¤¾à¤¸à¥à¤•à¤¿à¤‚à¤— à¤¸à¤‚à¤­à¤¾à¤µà¤¨à¤¾ à¤®à¥ˆà¤¨à¥à¤¯à¥à¤…à¤²à¥€ à¤¬à¤¢à¤¼à¤¾à¤¨à¥‡ à¤•à¤¾ à¤¸à¤®à¤°à¥à¤¥à¤¨ (**ğŸ’¡ à¤¸à¥à¤à¤¾à¤¯à¤¾ à¤—à¤¯à¤¾: 50%â€“70%, 80% à¤¸à¥‡ à¤…à¤§à¤¿à¤• à¤¨à¤¹à¥€à¤‚**) à¤”à¤° à¤¸à¤®à¤—à¥à¤° à¤®à¤¾à¤¸à¥à¤•à¤¿à¤‚à¤— à¤¦à¤° (à¤²à¤—à¤­à¤— 20%â€“25%) à¤•à¥‹ à¤¸à¤®à¤•à¤¾à¤²à¤¿à¤• à¤°à¥‚à¤ª à¤¸à¥‡ à¤¸à¤®à¤¾à¤¯à¥‹à¤œà¤¿à¤¤ à¤•à¤°à¤¨à¤¾ à¤¸à¤‚à¤­à¤µ à¤¹à¥ˆ, à¤œà¤¿à¤¸à¤¸à¥‡ à¤•à¤®à¤œà¥‹à¤° à¤œà¥à¤à¤¾à¤¨ à¤¬à¤¿à¤‚à¤¦à¥à¤“à¤‚ à¤ªà¤° à¤®à¥‰à¤¡à¤² à¤•à¥€ à¤¤à¤°à¥à¤• à¤•à¥à¤·à¤®à¤¤à¤¾ à¤•à¥‹ à¤²à¤•à¥à¤·à¤¿à¤¤ à¤°à¥‚à¤ª à¤¸à¥‡ à¤¸à¤¶à¤•à¥à¤¤ à¤•à¤¿à¤¯à¤¾ à¤œà¤¾ à¤¸à¤•à¥‡à¥¤

* **à¤¸à¥à¤µà¤šà¤¾à¤²à¤¿à¤¤ à¤°à¥‚à¤ª à¤¸à¥‡ à¤µà¤¿à¤°à¤¾à¤® à¤šà¤¿à¤¹à¥à¤¨ à¤”à¤° à¤—à¥ˆà¤°-à¤…à¤°à¥à¤¥à¤ªà¥‚à¤°à¥à¤£ token à¤¸à¥‡ à¤¬à¤šà¤¾à¤µ**  
  à¤®à¤¾à¤¸à¥à¤•à¤¿à¤‚à¤— à¤ªà¥à¤°à¤•à¥à¤°à¤¿à¤¯à¤¾ à¤•à¥‡ à¤¦à¥Œà¤°à¤¾à¤¨ à¤…à¤‚à¤—à¥à¤°à¥‡à¤œà¤¼à¥€-à¤šà¥€à¤¨à¥€ à¤µà¤¿à¤°à¤¾à¤® à¤šà¤¿à¤¹à¥à¤¨, à¤ªà¥à¤°à¤¤à¥€à¤• à¤”à¤° tokenizer à¤•à¥‡ à¤…à¤œà¥à¤à¤¾à¤¤ à¤…à¤•à¥à¤·à¤°à¥‹à¤‚ (`<unk>`) à¤•à¥‹ à¤¸à¥à¤µà¤šà¤¾à¤²à¤¿à¤¤ à¤°à¥‚à¤ª à¤¸à¥‡ à¤›à¥‹à¤¡à¤¼ à¤¦à¤¿à¤¯à¤¾ à¤œà¤¾à¤¤à¤¾ à¤¹à¥ˆ, à¤œà¤¿à¤¸à¤¸à¥‡ à¤¹à¤° mask span à¤µà¤¾à¤¸à¥à¤¤à¤µà¤¿à¤• à¤…à¤°à¥à¤¥à¤¯à¥à¤•à¥à¤¤ à¤‡à¤•à¤¾à¤ˆ à¤•à¥‡ à¤…à¤¨à¥à¤°à¥‚à¤ª à¤°à¤¹à¥‡à¥¤

* **à¤¸à¤¤à¤¤ à¤¯à¤¾ à¤“à¤µà¤°à¤²à¥ˆà¤ªà¤¿à¤‚à¤— span à¤•à¤¾ à¤¸à¥à¤µà¤šà¤¾à¤²à¤¿à¤¤ à¤µà¤¿à¤²à¤¯**  
  à¤†à¤¸à¤¨à¥à¤¨ à¤¯à¤¾ à¤“à¤µà¤°à¤²à¥ˆà¤ªà¤¿à¤‚à¤— à¤®à¤¾à¤¸à¥à¤• span à¤•à¥‹ à¤¸à¥à¤µà¤šà¤¾à¤²à¤¿à¤¤ à¤°à¥‚à¤ª à¤¸à¥‡ à¤®à¤¿à¤²à¤¾à¤¯à¤¾ à¤œà¤¾à¤¤à¤¾ à¤¹à¥ˆ, à¤¯à¤¹ à¤¸à¥à¤¨à¤¿à¤¶à¥à¤šà¤¿à¤¤ à¤•à¤°à¤¤à¤¾ à¤¹à¥ˆ à¤•à¤¿ à¤‰à¤¤à¥à¤ªà¤¨à¥à¤¨ `<extra_id_n>` à¤‡à¤¨à¤ªà¥à¤Ÿ/à¤†à¤‰à¤Ÿà¤ªà¥à¤Ÿ à¤œà¥‹à¤¡à¤¼à¥‡ à¤…à¤¨à¥à¤•à¥à¤°à¤®à¤¿à¤¤ à¤”à¤° à¤ªà¥à¤°à¤¶à¤¿à¤•à¥à¤·à¤£ à¤¸à¤¿à¤—à¥à¤¨à¤² à¤²à¤—à¤¾à¤¤à¤¾à¤° à¤¹à¥‹à¤‚, à¤”à¤° à¤¯à¤¹ à¤ªà¥‚à¤°à¥€ à¤¤à¤°à¤¹ à¤¸à¥‡ T5 à¤®à¥‚à¤² à¤¶à¥‹à¤§ à¤ªà¤¤à¥à¤° à¤•à¥‡ span corruption à¤ªà¥à¤°à¤¶à¤¿à¤•à¥à¤·à¤£ à¤µà¤¿à¤¨à¤¿à¤°à¥à¤¦à¥‡à¤¶ à¤•à¥‡ à¤…à¤¨à¥à¤•à¥‚à¤² à¤¹à¥‹à¥¤

* **à¤®à¤²à¥à¤Ÿà¥€-à¤¥à¥à¤°à¥‡à¤¡à¥‡à¤¡ à¤ªà¥ˆà¤°à¥‡à¤²à¤² à¤ªà¥à¤°à¥‹à¤¸à¥‡à¤¸à¤¿à¤‚à¤— à¤”à¤° à¤¸à¥à¤µà¤šà¤¾à¤²à¤¿à¤¤ à¤¨à¤®à¥‚à¤¨à¤¾ à¤°à¤¿à¤Ÿà¤°à¥à¤¨**  
  à¤¸à¤®à¤¾à¤¯à¥‹à¤œà¥à¤¯ à¤¥à¥à¤°à¥‡à¤¡ à¤¸à¤‚à¤–à¥à¤¯à¤¾ à¤•à¥‡ à¤¸à¤¾à¤¥ à¤ªà¥ˆà¤°à¥‡à¤²à¤² à¤°à¥‚à¤ª à¤¸à¥‡ à¤¨à¤®à¥‚à¤¨à¥‡ à¤‰à¤¤à¥à¤ªà¤¨à¥à¤¨ à¤•à¤°à¤¨à¥‡ à¤•à¤¾ à¤¸à¤®à¤°à¥à¤¥à¤¨, à¤¬à¤¡à¤¼à¥‡ à¤ªà¥ˆà¤®à¤¾à¤¨à¥‡ à¤ªà¤° à¤•à¥‰à¤°à¥à¤ªà¤¸ à¤ªà¥à¤°à¥‹à¤¸à¥‡à¤¸à¤¿à¤‚à¤— à¤•à¥€ à¤¦à¤•à¥à¤·à¤¤à¤¾ à¤•à¥‹ à¤•à¤¾à¤«à¥€ à¤¬à¤¢à¤¼à¤¾à¤¤à¤¾ à¤¹à¥ˆ; à¤¸à¤­à¥€ à¤‰à¤¤à¥à¤ªà¤¨à¥à¤¨ à¤¨à¤®à¥‚à¤¨à¥‡ à¤¸à¥à¤µà¤šà¤¾à¤²à¤¿à¤¤ à¤°à¥‚à¤ª à¤¸à¥‡ à¤µà¤¾à¤ªà¤¸ à¤²à¥Œà¤Ÿà¤¾à¤ à¤œà¤¾à¤¤à¥‡ à¤¹à¥ˆà¤‚ à¤”à¤° à¤®à¥‚à¤² à¤…à¤¨à¥à¤•à¥à¤°à¤® à¤®à¥‡à¤‚ à¤à¤•à¤¤à¥à¤° à¤•à¤¿à¤ à¤œà¤¾à¤¤à¥‡ à¤¹à¥ˆà¤‚, à¤…à¤‚à¤¤à¤¤à¤ƒ à¤‰à¤¨à¥à¤¹à¥‡à¤‚ à¤ªà¥à¤°à¤¶à¤¿à¤•à¥à¤·à¤£ à¤•à¥‡ à¤²à¤¿à¤ à¤¸à¥€à¤§à¥‡ à¤‰à¤ªà¤¯à¥‹à¤— à¤¯à¥‹à¤—à¥à¤¯ à¤®à¤¾à¤¨à¤•à¥€à¤•à¥ƒà¤¤ JSONL à¤¡à¥‡à¤Ÿà¤¾à¤¸à¥‡à¤Ÿ à¤•à¥‡ à¤°à¥‚à¤ª à¤®à¥‡à¤‚ à¤†à¤‰à¤Ÿà¤ªà¥à¤Ÿ à¤•à¤¿à¤¯à¤¾ à¤œà¤¾à¤¤à¤¾ à¤¹à¥ˆà¥¤

**à¤¸à¤‚à¤¦à¤°à¥à¤­-à¤¸à¤‚à¤µà¥‡à¤¦à¤¨à¤¶à¥€à¤² à¤¡à¥‡à¤Ÿà¤¾ à¤¨à¤¿à¤°à¥à¤®à¤¾à¤£, à¤•à¥€à¤µà¤°à¥à¤¡ à¤ªà¥à¤°à¤¾à¤¥à¤®à¤¿à¤•à¤¤à¤¾ à¤®à¤¾à¤¸à¥à¤•à¤¿à¤‚à¤—, à¤¯à¤¾à¤¦à¥ƒà¤šà¥à¤›à¤¿à¤• span à¤®à¤¾à¤¸à¥à¤• à¤”à¤° à¤œà¤¾à¤¨à¤¬à¥‚à¤à¤•à¤° à¤¨à¤¿à¤°à¥à¤®à¤¿à¤¤ â€œà¤…à¤¤à¥à¤¯à¤§à¤¿à¤• à¤¸à¥‚à¤šà¤¨à¤¾ à¤¹à¥à¤°à¤¾à¤¸â€ à¤ªà¤°à¤¿à¤¦à¥ƒà¤¶à¥à¤¯à¥‹à¤‚ à¤•à¥‡ à¤®à¤¾à¤§à¥à¤¯à¤® à¤¸à¥‡, à¤®à¥‰à¤¡à¤² à¤¸à¤¬à¤¸à¥‡ à¤ªà¥à¤°à¤¤à¤¿à¤•à¥‚à¤² à¤‡à¤¨à¤ªà¥à¤Ÿ à¤¸à¥à¤¥à¤¿à¤¤à¤¿à¤¯à¥‹à¤‚ à¤®à¥‡à¤‚ à¤­à¥€ à¤ªà¥‡à¤¶à¥‡à¤µà¤° à¤…à¤°à¥à¤¥ à¤•à¥€ à¤¸à¤Ÿà¥€à¤• à¤¸à¤®à¤ à¤”à¤° à¤ªà¥à¤¨à¤°à¥à¤¨à¤¿à¤°à¥à¤®à¤¾à¤£ à¤¬à¤¨à¤¾à¤ à¤°à¤– à¤¸à¤•à¤¤à¤¾ à¤¹à¥ˆà¥¤**  

> à¤‰à¤ªà¤¯à¥à¤•à¥à¤¤ à¤ªà¤°à¤¿à¤¦à¥ƒà¤¶à¥à¤¯: à¤šà¤¿à¤•à¤¿à¤¤à¥à¤¸à¤¾, à¤•à¤¾à¤¨à¥‚à¤¨à¥€, à¤¤à¤•à¤¨à¥€à¤•à¥€ à¤¦à¤¸à¥à¤¤à¤¾à¤µà¥‡à¤œà¤¼ à¤†à¤¦à¤¿ à¤à¤¸à¥‡ à¤ªà¥‡à¤¶à¥‡à¤µà¤° à¤•à¥‰à¤°à¥à¤ªà¤¸ à¤œà¤¿à¤¨à¤®à¥‡à¤‚ à¤¸à¤Ÿà¥€à¤• à¤…à¤°à¥à¤¥ à¤¸à¤®à¤ à¤•à¥€ à¤†à¤µà¤¶à¥à¤¯à¤•à¤¤à¤¾ à¤¹à¥‹à¤¤à¥€ à¤¹à¥ˆà¥¤

### â—ï¸à¤ªà¥à¤°à¤¶à¤¿à¤•à¥à¤·à¤£ à¤•à¥‡ à¤²à¤¿à¤ à¤¸à¤¾à¤µà¤§à¤¾à¤¨à¤¿à¤¯à¤¾à¤‚
* **à¤®à¥‰à¤¡à¤² à¤•à¥‹ à¤¸à¤®à¤¯ à¤¸à¥‡ à¤ªà¤¹à¤²à¥‡ à¤°à¥à¤•à¤¨à¥‡ à¤¸à¥‡ à¤°à¥‹à¤•à¤¨à¤¾**: à¤ªà¥à¤°à¥€-à¤ªà¥à¤°à¥‹à¤¸à¥‡à¤¸à¤¿à¤‚à¤— à¤•à¥‡ à¤¬à¤¾à¤¦, T5 à¤®à¥‰à¤¡à¤² à¤®à¥‡à¤‚ à¤²à¥‰à¤¸ (Loss) à¤•à¥‡ à¤§à¥€à¤°à¥‡-à¤§à¥€à¤°à¥‡ à¤—à¤¿à¤°à¤¨à¥‡ à¤¯à¤¾ à¤¸à¥à¤¥à¤¾à¤¨à¥€à¤¯ à¤‰à¤¤à¤¾à¤°-à¤šà¤¢à¤¼à¤¾à¤µ à¤•à¤¾ à¤­à¥à¤°à¤® à¤¹à¥‹ à¤¸à¤•à¤¤à¤¾ à¤¹à¥ˆ, à¤œà¤¿à¤¸à¤¸à¥‡ à¤¸à¤¿à¤¸à¥à¤Ÿà¤® à¤—à¤²à¤¤à¥€ à¤¸à¥‡ à¤ªà¥à¤°à¤¶à¤¿à¤•à¥à¤·à¤£ à¤•à¥‹ à¤œà¤²à¥à¤¦à¥€ à¤°à¥‹à¤• à¤¸à¤•à¤¤à¤¾ à¤¹à¥ˆà¥¤
* **à¤…à¤­à¤¿à¤¸à¤°à¤£ (Convergence) à¤¨à¤¿à¤°à¥à¤£à¤¯ à¤•à¤¾ à¤¸à¥à¤à¤¾à¤µ**: à¤ªà¥à¤°à¤¶à¤¿à¤•à¥à¤·à¤£ à¤•à¤¾ à¤¸à¤®à¤¯ à¤¬à¤¢à¤¼à¤¾à¤¨à¥‡ à¤•à¥€ à¤¸à¤¿à¤«à¤¾à¤°à¤¿à¤¶ à¤•à¥€ à¤œà¤¾à¤¤à¥€ à¤¹à¥ˆ, à¤”à¤° à¤®à¥‰à¤¡à¤² à¤•à¥‡ à¤…à¤­à¤¿à¤¸à¤°à¤£ à¤•à¤¾ à¤†à¤•à¤²à¤¨ à¤‡à¤¸ à¤†à¤§à¤¾à¤° à¤ªà¤° à¤•à¤°à¥‡à¤‚ à¤•à¤¿ à¤•à¥à¤¯à¤¾ à¤•à¤ˆ à¤šà¤°à¤£à¥‹à¤‚ à¤®à¥‡à¤‚ à¤²à¥‰à¤¸ à¤²à¤—à¤¾à¤¤à¤¾à¤° à¤”à¤° à¤¸à¥à¤¥à¤¿à¤°à¤¤à¤¾ à¤¸à¥‡ à¤—à¤¿à¤° à¤°à¤¹à¤¾ à¤¹à¥ˆà¥¤ à¤¯à¤¦à¤¿ à¤ªà¥à¤°à¤¶à¤¿à¤•à¥à¤·à¤£ à¤•à¤¾ à¤¸à¤®à¤¯ à¤…à¤ªà¤°à¥à¤¯à¤¾à¤ªà¥à¤¤ à¤¹à¥ˆ, à¤¤à¥‹ à¤¬à¤¹à¤¾à¤²à¥€ à¤•à¤¾ à¤ªà¥à¤°à¤­à¤¾à¤µ à¤¬à¤¹à¥à¤¤ à¤•à¤® à¤¹à¥‹ à¤¸à¤•à¤¤à¤¾ à¤¹à¥ˆà¥¤

### ğŸ“Šà¤ªà¥à¤°à¤­à¤¾à¤µ à¤•à¤¾ à¤®à¥‚à¤²à¥à¤¯à¤¾à¤‚à¤•à¤¨
mT5-base à¤®à¤¾à¤¨à¤• à¤®à¥‰à¤¡à¤² à¤•à¥‡ à¤¸à¤¾à¤¥ à¤ªà¥à¤°à¤¾à¤°à¤‚à¤­à¤¿à¤• à¤ªà¤°à¥€à¤•à¥à¤·à¤£ à¤¤à¥à¤²à¤¨à¤¾ à¤•à¥‡ à¤…à¤¨à¥à¤¸à¤¾à¤°:
* **à¤®à¤¾à¤¨à¤• à¤®à¥‰à¤¡à¤² à¤•à¤¾ à¤ªà¥à¤°à¤¦à¤°à¥à¤¶à¤¨**: à¤ªà¥‡à¤¶à¥‡à¤µà¤° à¤¡à¥‹à¤®à¥‡à¤¨ à¤¶à¤¬à¥à¤¦à¤¾à¤µà¤²à¥€ à¤¬à¤¹à¤¾à¤²à¥€ à¤¦à¤° 60% à¤¸à¥‡ à¤•à¤® à¤¹à¥‹à¤¨à¥‡ à¤•à¤¾ à¤…à¤¨à¥à¤®à¤¾à¤¨ à¤¹à¥ˆ, à¤¶à¥‡à¤· 40% à¤ªà¤°à¤¿à¤£à¤¾à¤® à¤¤à¤¾à¤°à¥à¤•à¤¿à¤• à¤°à¥‚à¤ª à¤¸à¥‡ à¤­à¥à¤°à¤®à¤¿à¤¤ à¤¹à¥ˆà¤‚ à¤”à¤° à¤µà¥à¤¯à¤¾à¤µà¤¸à¤¾à¤¯à¤¿à¤• à¤‰à¤ªà¤¯à¥‹à¤— à¤•à¥‡ à¤²à¤¿à¤ à¤²à¤—à¤­à¤— à¤…à¤¸à¥à¤µà¥€à¤•à¤¾à¤°à¥à¤¯ à¤¹à¥ˆà¤‚à¥¤
* **à¤‡à¤¸ à¤ªà¥à¤°à¥‹à¤œà¥‡à¤•à¥à¤Ÿ à¤•à¥‡ à¤¸à¥à¤§à¤¾à¤° à¤•à¥‡ à¤¬à¤¾à¤¦**: à¤ªà¥‡à¤¶à¥‡à¤µà¤° à¤¶à¤¬à¥à¤¦à¤¾à¤µà¤²à¥€ à¤¬à¤¹à¤¾à¤²à¥€ à¤¦à¤° à¤•à¤¾ à¤…à¤¨à¥à¤®à¤¾à¤¨ 85% à¤¤à¤• à¤ªà¤¹à¥à¤‚à¤š à¤—à¤¯à¤¾ à¤¹à¥ˆà¥¤ à¤¶à¥‡à¤· 15% à¤¤à¥à¤°à¥à¤Ÿà¤¿à¤¯à¥‹à¤‚ à¤®à¥‡à¤‚ à¤¸à¥‡ à¤…à¤§à¤¿à¤•à¤¾à¤‚à¤¶ à¤¸à¤®à¤¾à¤¨ à¤…à¤°à¥à¤¥ à¤µà¤¾à¤²à¥‡ à¤¶à¤¬à¥à¤¦à¥‹à¤‚ à¤•à¤¾ à¤ªà¥à¤°à¤¤à¤¿à¤¸à¥à¤¥à¤¾à¤ªà¤¨ à¤¹à¥ˆà¤‚, à¤œà¤¿à¤¸à¤¸à¥‡ à¤Ÿà¥‡à¤•à¥à¤¸à¥à¤Ÿ à¤•à¥€ à¤¸à¤®à¤—à¥à¤° à¤ªà¤ à¤¨à¥€à¤¯à¤¤à¤¾ à¤”à¤° à¤¤à¤¾à¤°à¥à¤•à¤¿à¤• à¤¨à¤¿à¤°à¤‚à¤¤à¤°à¤¤à¤¾ à¤®à¥‡à¤‚ à¤•à¤¾à¤«à¥€ à¤¸à¥à¤§à¤¾à¤° à¤¹à¥à¤† à¤¹à¥ˆà¥¤

### âš ï¸à¤‰à¤ªà¤¯à¥‹à¤— à¤•à¥€ à¤¸à¥€à¤®à¤¾à¤à¤‚
* **à¤¸à¤‚à¤¦à¤°à¥à¤­ à¤µà¤¿à¤–à¤‚à¤¡à¤¨ (Context Fragmentation) à¤•à¥€ à¤¸à¥€à¤®à¤¾**: à¤®à¥‰à¤¡à¤² à¤¦à¥à¤µà¤¾à¤°à¤¾ à¤à¤• à¤¬à¤¾à¤° à¤®à¥‡à¤‚ à¤¸à¤‚à¤¸à¤¾à¤§à¤¿à¤¤ à¤Ÿà¥‡à¤•à¥à¤¸à¥à¤Ÿ à¤•à¥€ à¤²à¤‚à¤¬à¤¾à¤ˆ à¤¸à¥€à¤®à¤¿à¤¤ à¤¹à¥‹à¤¨à¥‡ à¤•à¥‡ à¤•à¤¾à¤°à¤£, à¤²à¤‚à¤¬à¥‡ à¤¦à¤¸à¥à¤¤à¤¾à¤µà¥‡à¤œà¥‹à¤‚ à¤•à¥‹ à¤•à¤¾à¤Ÿà¤¤à¥‡ à¤¸à¤®à¤¯ à¤¸à¤‚à¤¦à¤°à¥à¤­ à¤•à¥€ à¤œà¤¾à¤¨à¤•à¤¾à¤°à¥€ à¤Ÿà¥‚à¤Ÿ à¤¸à¤•à¤¤à¥€ à¤¹à¥ˆ, à¤œà¤¿à¤¸à¤¸à¥‡ à¤•à¥à¤› à¤•à¥à¤°à¥‰à¤¸-à¤ªà¥ˆà¤°à¤¾à¤—à¥à¤°à¤¾à¤« à¤…à¤°à¥à¤¥ à¤ªà¥‚à¤°à¥€ à¤¤à¤°à¤¹ à¤¸à¥‡ à¤•à¥ˆà¤ªà¥à¤šà¤° à¤¨à¤¹à¥€à¤‚ à¤¹à¥‹ à¤ªà¤¾à¤¤à¥‡à¥¤ à¤¸à¤‚à¤¦à¤°à¥à¤­ à¤•à¥‡ à¤•à¥à¤› à¤¹à¤¿à¤¸à¥à¤¸à¥‹à¤‚ à¤•à¥‹ à¤µà¤¾à¤ªà¤¸ à¤­à¥‡à¤œà¤•à¤° à¤ªà¥à¤¨: à¤ªà¥à¤°à¤¶à¤¿à¤•à¥à¤·à¤¿à¤¤ à¤•à¤°à¤¨à¥‡ à¤•à¥€ à¤¸à¤¿à¤«à¤¾à¤°à¤¿à¤¶ à¤•à¥€ à¤œà¤¾à¤¤à¥€ à¤¹à¥ˆà¥¤
* **à¤à¤²à¥à¤—à¥‹à¤°à¤¿à¤¥à¤® à¤•à¥€ à¤¸à¥€à¤®à¤¾à¤à¤‚**: à¤šà¥‚à¤‚à¤•à¤¿ T5 à¤®à¥‰à¤¡à¤² à¤•à¥€ à¤¬à¤¹à¤¾à¤²à¥€ à¤¸à¤¾à¤‚à¤–à¥à¤¯à¤¿à¤•à¥€à¤¯ à¤¸à¤‚à¤­à¤¾à¤µà¥à¤¯à¤¤à¤¾ à¤à¤²à¥à¤—à¥‹à¤°à¤¿à¤¦à¤® à¤ªà¤° à¤†à¤§à¤¾à¤°à¤¿à¤¤ à¤¹à¥ˆ, à¤‡à¤¸à¤²à¤¿à¤ à¤œà¤Ÿà¤¿à¤² à¤Ÿà¥‡à¤•à¥à¤¸à¥à¤Ÿ à¤•à¥‹ à¤¸à¤‚à¤­à¤¾à¤²à¤¤à¥‡ à¤¸à¤®à¤¯ 100% à¤¸à¤Ÿà¥€à¤•à¤¤à¤¾ à¤•à¥€ à¤—à¤¾à¤°à¤‚à¤Ÿà¥€ à¤¦à¥‡à¤¨à¤¾ à¤…à¤¸à¤‚à¤­à¤µ à¤¹à¥ˆà¥¤
* **à¤¡à¥‹à¤®à¥‡à¤¨ à¤¨à¤¿à¤°à¥à¤­à¤°à¤¤à¤¾**: à¤¬à¤¹à¤¾à¤²à¥€ à¤•à¤¾ à¤ªà¥à¤°à¤­à¤¾à¤µ à¤ªà¥‚à¤°à¥à¤µ-à¤¨à¤¿à¤°à¥à¤§à¤¾à¤°à¤¿à¤¤ à¤µà¤¿à¤¶à¥‡à¤·à¤œà¥à¤ à¤¶à¤¬à¥à¤¦à¤¾à¤µà¤²à¥€ à¤•à¥‡ à¤•à¤µà¤°à¥‡à¤œ à¤”à¤° à¤—à¤¹à¤°à¤¾à¤ˆ à¤ªà¤° à¤…à¤¤à¥à¤¯à¤§à¤¿à¤• à¤¨à¤¿à¤°à¥à¤­à¤° à¤•à¤°à¤¤à¤¾ à¤¹à¥ˆà¥¤

### ğŸŒŒà¤­à¤µà¤¿à¤·à¥à¤¯ à¤•à¥€ à¤µà¤¿à¤•à¤¾à¤¸ à¤¯à¥‹à¤œà¤¨à¤¾à¤à¤‚
* à¤¸à¥à¤µà¤šà¤¾à¤²à¤¿à¤¤ à¤•à¤®à¥€ à¤•à¥€ à¤ªà¤¹à¤šà¤¾à¤¨ (Automatic Defect Sensing):
à¤Ÿà¥‹à¤•à¤¨à¤¾à¤‡à¤œà¤¼à¤° à¤•à¥‡ "à¤…à¤¸à¤¾à¤®à¤¾à¤¨à¥à¤¯ à¤Ÿà¥à¤•à¤¡à¤¼à¥‹à¤‚" à¤•à¥‹ à¤›à¤¿à¤ªà¥‡ à¤¹à¥à¤ à¤¸à¤‚à¤•à¥‡à¤¤à¥‹à¤‚ à¤•à¥‡ à¤°à¥‚à¤ª à¤®à¥‡à¤‚ à¤‰à¤ªà¤¯à¥‹à¤— à¤•à¤°à¤¨à¤¾à¥¤ à¤œà¤¬ OCR à¤ªà¤¹à¤šà¤¾à¤¨ à¤®à¥‡à¤‚ à¤—à¤‚à¤­à¥€à¤° à¤µà¤¿à¤¸à¤‚à¤—à¤¤à¤¿ à¤¹à¥‹à¤¤à¥€ à¤¹à¥ˆ, à¤¤à¥‹ à¤®à¥‰à¤¡à¤² à¤Ÿà¥‹à¤•à¤¨ à¤…à¤¨à¥à¤•à¥à¤°à¤® à¤•à¥‡ à¤…à¤¸à¤¾à¤®à¤¾à¤¨à¥à¤¯ à¤‰à¤¤à¤¾à¤°-à¤šà¤¢à¤¼à¤¾à¤µ à¤•à¥‡ à¤®à¤¾à¤§à¥à¤¯à¤® à¤¸à¥‡ à¤¸à¥à¤µà¤šà¤¾à¤²à¤¿à¤¤ à¤°à¥‚à¤ª à¤¸à¥‡ à¤¸à¤¿à¤®à¥‡à¤‚à¤Ÿà¤¿à¤• à¤¬à¥à¤°à¥‡à¤• à¤•à¤¾ à¤ªà¤¤à¤¾ à¤²à¤—à¤¾ à¤¸à¤•à¥‡à¤—à¤¾à¥¤
* à¤¸à¥à¤µà¤šà¤¾à¤²à¤¿à¤¤ à¤¸à¤¿à¤®à¥‡à¤‚à¤Ÿà¤¿à¤• à¤¸à¤‚à¤°à¥‡à¤–à¤£:
à¤®à¥ˆà¤¨à¥à¤¯à¥à¤…à¤² à¤°à¥‚à¤ª à¤¸à¥‡ à¤•à¤¨à¥‡à¤•à¥à¤¶à¤¨ à¤¬à¤¿à¤‚à¤¦à¥ à¤¨à¤¿à¤°à¥à¤¦à¤¿à¤·à¥à¤Ÿ à¤•à¤¿à¤ à¤¬à¤¿à¤¨à¤¾, OCR à¤•à¥à¤·à¤¤à¤¿à¤—à¥à¤°à¤¸à¥à¤¤ à¤Ÿà¥‡à¤•à¥à¤¸à¥à¤Ÿ à¤•à¥€ à¤à¤‚à¤¡-à¤Ÿà¥‚-à¤à¤‚à¤¡ à¤®à¤°à¤®à¥à¤®à¤¤ à¤ªà¥à¤°à¤¾à¤ªà¥à¤¤ à¤•à¤°à¤¨à¤¾à¥¤

[Demo](#Demo)

---
<a name="korean"></a>
## í•œêµ­ì–´ï¼ˆê¸°ê³„ ë²ˆì—­ï¼‰
**T5-Refiner-DomainFocus**ëŠ” **ì‚¬ì „ í•™ìŠµ ë‹¨ê³„**ì˜ ì „ëµì  ìµœì í™”ë¥¼ í†µí•´ ëª¨ë¸ì— ë‚´ì¬ì ì¸ 'ì˜ë¯¸ì  íšŒë³µíƒ„ë ¥ì„±(Semantic Resilience)'ì„ ë¶€ì—¬í•˜ì—¬, **í…ìŠ¤íŠ¸ ê²°ì†ì„ ë”ìš± ê²¬ê³ í•˜ê²Œ ì²˜ë¦¬í•˜ê³  ë„ë©”ì¸ ì „ë¬¸ ì§€ì‹ì„ ì£¼ì…í•  ìˆ˜ ìˆë„ë¡ ì„¤ê³„ë˜ì—ˆìŠµë‹ˆë‹¤.**

### ğŸ“– í”„ë¡œì íŠ¸ ë°°ê²½
**ì˜ë£Œ ê¸°ë¡ ë””ì§€í„¸í™”** ê³¼ì •ì—ì„œ **OCR(ê´‘í•™ ë¬¸ì ì¸ì‹)**ì€ ì¢…ì´ ì†ìƒ, ì§ì¸ ê°€ë ¤ì§ ë“±ì˜ ì‚¬ìœ ë¡œ í•µì‹¬ ìš©ì–´ì— 'ë¬¸ì ê²°ì†'ì´ ë°œìƒí•˜ëŠ” ê²½ìš°ê°€ ë¹ˆë²ˆí•©ë‹ˆë‹¤.
ê¸°ì¡´ì˜ **T5** ë˜ëŠ” **mT5** ëª¨ë¸(í†µì¹­ T5)ì€ ì´ëŸ¬í•œ ì†ìƒëœ í…ìŠ¤íŠ¸ë¥¼ ì²˜ë¦¬í•  ë•Œ ë‘ ê°€ì§€ ì£¼ìš” ë¬¸ì œë¥¼ ê²ªìŠµë‹ˆë‹¤:
* **ëœë¤ ë§ˆìŠ¤í‚¹ì˜ í•œê³„**: ëª¨ë¸ì´ ì–´ê·¼ì— ê¸°ë°˜í•œ 'ë‹¨ì–´ ì¶”ì¸¡'ë§Œ í•™ìŠµí•˜ê²Œ ë˜ì–´, ì™„ì „í•œ ì˜ë£Œ ê°œë…ì„ ì§„ì •ìœ¼ë¡œ ì´í•´í•˜ì§€ ëª»í•¨.
* **í† í°í™” ì–´ê¸‹ë‚¨ ë¬¸ì œ**: ìš©ì–´ì˜ ê¸€ìê°€ ìœ ì‹¤ë  ë•Œ í† í°ë¼ì´ì €ê°€ ì´ë¥¼ ì˜ë¯¸ ì—†ëŠ” íŒŒí¸ìœ¼ë¡œ ìª¼ê°œë²„ë ¤ ëª¨ë¸ì´ ì˜ë¯¸ì  ì¤‘ì‹¬ì„ ìƒê²Œ ë¨.

### âœ… í˜„ì¬ í•µì‹¬ ê¸°ëŠ¥
ë³¸ í”„ë¡œì íŠ¸ëŠ” í˜„ì¬ ë³µì¡í•œ í•˜ë“œì½”ë”© ê·œì¹™ì— ì˜ì¡´í•˜ì§€ ì•Šê³ , ë°ì´í„° ì „ì²˜ë¦¬, ë§ˆìŠ¤í‚¹ ì „ëµ ë° ì‹¤í–‰ í”„ë¡œì„¸ìŠ¤ë¥¼ ìµœì í™”í•˜ì—¬ ëª¨ë¸ ì„±ëŠ¥ì„ í–¥ìƒì‹œí‚¤ëŠ” ë°©ì‹ìœ¼ë¡œ ì„¤ê³„ë˜ì—ˆìŠµë‹ˆë‹¤.

**ì£¼ì˜:** ë³¸ í”„ë¡œì íŠ¸ëŠ” T5 ìì²´ í† í¬ë‚˜ì´ì €ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤ã€‚

* **ê¸´ í…ìŠ¤íŠ¸ë¥¼ ë‹¨ë½ ë° ë¬¸ì¥ ë¶€í˜¸ ê¸°ì¤€ìœ¼ë¡œ ë‚˜ëˆ„ì–´ í•™ìŠµ ìƒ˜í”Œ ìƒì„±**  
  ì†Œì„¤, ë¬¸ì„œ ë˜ëŠ” ì „ë¬¸ ì½”í¼ìŠ¤ì—ì„œ ìë™ìœ¼ë¡œ ìƒ˜í”Œì„ ìƒì„±í•˜ë©°, ë‹¨ë½ ê²½ê³„ì™€ í•œì˜ ë¬¸ì¥ ë¶€í˜¸ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ë¶„í• í•©ë‹ˆë‹¤. ì´ì „ í…ìŠ¤íŠ¸ì˜ ë§ˆì§€ë§‰ ë¬¸ì¥ì„ ì»¨í…ìŠ¤íŠ¸ ì ‘ë‘ì‚¬ë¡œ ìœ ì§€í•˜ì—¬ ìƒ˜í”Œì˜ ì˜ë¯¸ ì—°ì†ì„±ì„ ë³´ì¥í•˜ê³ , ë‹¨ë½ ê°„ ì˜ë¯¸ ë‹¨ì ˆì„ ì™„í™”í•˜ë©°, ë‹¨ì¼ ìƒ˜í”Œ ê¸¸ì´ë¥¼ ëª¨ë¸ì˜ ì»¨í…ìŠ¤íŠ¸ ì°½ì— ë§ê²Œ ì œí•œí•©ë‹ˆë‹¤.

* **ì „ë¬¸ê°€ ë‹¨ì–´ ì‚¬ì „ì„ í™œìš©í•œ ì›ìì  ë§ˆìŠ¤í‚¹**  
  ì‚¬ìš©ì ì •ì˜ ë„ë©”ì¸ ë‹¨ì–´ ì‚¬ì „ì„ í™œìš©í•˜ì—¬, ì „ë¬¸ ìš©ì–´(ì˜ˆ: ê¸‰ì„± ì „ë²½ ì‹¬ê·¼ê²½ìƒ‰, ê²½í”¼ì  ê´€ìƒë™ë§¥ ì¤‘ì¬ìˆ )ë¥¼ ë¶„ë¦¬í•  ìˆ˜ ì—†ëŠ” ë‹¨ìœ„ë¡œ ë§ˆìŠ¤í‚¹í•©ë‹ˆë‹¤. ì´ë¥¼ í†µí•´ ë¬¸ì ë‹¨ìœ„ ì¶”ì¸¡ ê²½ë¡œë¥¼ ì°¨ë‹¨í•˜ê³ , ëª¨ë¸ì´ ì»¨í…ìŠ¤íŠ¸ ì˜ë¯¸ì™€ ë„ë©”ì¸ ë…¼ë¦¬ì— ê¸°ë°˜í•˜ì—¬ ë³µì›í•˜ë„ë¡ ê°•ì œí•©ë‹ˆë‹¤.

* **ìˆ˜ë™ ì œì–´ ê°€ëŠ¥í•œ ê°•í™” ë§ˆìŠ¤í‚¹ ì „ëµ**  
  ê³ ë‚œë„ ìš©ì–´ì— ëŒ€í•´ ë§ˆìŠ¤í‚¹ í™•ë¥ ì„ ìˆ˜ë™ìœ¼ë¡œ ì¡°ì • ê°€ëŠ¥í•˜ë©° (**ğŸ’¡ê¶Œì¥ 50%â€“70%, 80% ì´ìƒì€ ë¹„ì¶”ì²œ**), ì „ì²´ ë§ˆìŠ¤í‚¹ ë¹„ìœ¨(ì•½ 20%â€“25%)ë„ ë™ì‹œ ì¡°ì •í•  ìˆ˜ ìˆì–´, ëª¨ë¸ì´ ì•½í•œ ì§€ì‹ ì˜ì—­ì—ì„œ ì¶”ë¡  ëŠ¥ë ¥ì„ ê°•í™”í•˜ë„ë¡ ì„¤ê³„ë˜ì—ˆìŠµë‹ˆë‹¤.

* **ë¬¸ì¥ ë¶€í˜¸ ë° ì˜ë¯¸ ì—†ëŠ” í† í° ìë™ íšŒí”¼**  
  ë§ˆìŠ¤í‚¹ ê³¼ì •ì—ì„œ í•œì˜ ë¬¸ì¥ ë¶€í˜¸, íŠ¹ìˆ˜ ê¸°í˜¸, tokenizerì—ì„œ ì¸ì‹ë˜ì§€ ì•ŠëŠ” ë¬¸ì(`<unk>`)ë¥¼ ìë™ìœ¼ë¡œ íšŒí”¼í•˜ì—¬, ëª¨ë“  ë§ˆìŠ¤í¬ spanì´ ìœ íš¨í•œ ì˜ë¯¸ ë‹¨ìœ„ì™€ ëŒ€ì‘ë˜ë„ë¡ ë³´ì¥í•©ë‹ˆë‹¤.

* **ì—°ì† ë˜ëŠ” ì¤‘ì²©ëœ span ìë™ ë³‘í•©**  
  ì¸ì ‘í•˜ê±°ë‚˜ ì¤‘ì²©ëœ ë§ˆìŠ¤í¬ spanì„ ìë™ìœ¼ë¡œ ë³‘í•©í•˜ì—¬ `<extra_id_n>` ì…ë ¥/ì¶œë ¥ ìˆœì„œë¥¼ ì¼ê´€ë˜ê²Œ ìœ ì§€í•˜ê³ , í•™ìŠµ ì‹ í˜¸ë¥¼ ì—°ì†ì ìœ¼ë¡œ ì œê³µí•˜ë©° T5 ì› ë…¼ë¬¸ì˜ span corruption í•™ìŠµ ê·œê²©ì„ ì™„ì „íˆ ì¤€ìˆ˜í•©ë‹ˆë‹¤.

* **ë©€í‹°ìŠ¤ë ˆë“œ ë³‘ë ¬ ì²˜ë¦¬ ë° ìƒ˜í”Œ ìë™ íšŒìˆ˜**  
  ìŠ¤ë ˆë“œ ìˆ˜ ì¡°ì •ì´ ê°€ëŠ¥í•œ ë³‘ë ¬ ì²˜ë¦¬ë¡œ ëŒ€ê·œëª¨ ì½”í¼ìŠ¤ ì²˜ë¦¬ ì†ë„ë¥¼ í¬ê²Œ í–¥ìƒì‹œí‚¤ë©°, ìƒì„±ëœ ëª¨ë“  ìƒ˜í”Œì„ ìë™ìœ¼ë¡œ íšŒìˆ˜í•˜ê³  ì›ë³¸ ìˆœì„œëŒ€ë¡œ ì •ë ¬í•˜ì—¬, ìµœì¢…ì ìœ¼ë¡œ í•™ìŠµì— ë°”ë¡œ ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” í‘œì¤€ JSONL ë°ì´í„°ì…‹ìœ¼ë¡œ ì¶œë ¥í•©ë‹ˆë‹¤.

**ì»¨í…ìŠ¤íŠ¸ ì¸ì§€ ê¸°ë°˜ ë°ì´í„° êµ¬ì„±, í‚¤ì›Œë“œ ìš°ì„  ë§ˆìŠ¤í‚¹, ëœë¤ span ë§ˆìŠ¤í‚¹ ë° ì˜ë„ì ìœ¼ë¡œ ìƒì„±ëœ 'ê·¹ë‹¨ì  ì •ë³´ ê²°ì†' ì‹œë‚˜ë¦¬ì˜¤ë¥¼ í†µí•´, ëª¨ë¸ì€ ê°€ì¥ ë¶ˆë¦¬í•œ ì…ë ¥ ì¡°ê±´ì—ì„œë„ ì „ë¬¸ ìš©ì–´ì˜ ì •í™•í•œ ì˜ë¯¸ ì´í•´ì™€ ë³µì›ì´ ê°€ëŠ¥í•˜ë„ë¡ í•™ìŠµë©ë‹ˆë‹¤.**  

> ì ìš© ë¶„ì•¼: ì˜í•™, ë²•ë¥ , ê¸°ìˆ  ë¬¸ì„œ ë“± ì •í™•í•œ ì˜ë¯¸ ì´í•´ê°€ ìš”êµ¬ë˜ëŠ” ì „ë¬¸ ì½”í¼ìŠ¤.

### â—ï¸ í•™ìŠµ ì‹œ ì£¼ì˜ì‚¬í•­
* **ëª¨ë¸ ì¡°ê¸° ì¢…ë£Œ ë°©ì§€**: ì „ì²˜ë¦¬ ì´í›„ T5 ëª¨ë¸ì€ Loss í•˜ë½ì´ ëŠë ¤ì§€ê±°ë‚˜ êµ­ì†Œì ì¸ ë³€ë™ì´ ë°œìƒí•˜ëŠ” ê°€ì§œ ì •ì²´ í˜„ìƒì´ ë‚˜íƒ€ë‚  ìˆ˜ ìˆìœ¼ë©°, ì´ë¡œ ì¸í•´ ì‹œìŠ¤í…œì´ í•™ìŠµì„ ì˜ëª» ì¡°ê¸° ì¢…ë£Œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
* **ìˆ˜ë ´ íŒë‹¨ ê¶Œì¥**: í•™ìŠµ ì‹œê°„ì„ ëŠ˜ë¦¬ê³ , ì—¬ëŸ¬ ë‹¨ê³„ì—ì„œ Lossê°€ ì§€ì†ì ìœ¼ë¡œ ì•ˆì •ë˜ê²Œ í•˜ë½í•˜ëŠ”ì§€ë¥¼ ì¢…í•©ì ìœ¼ë¡œ íŒë‹¨í•˜ì—¬ ìˆ˜ë ´ ì—¬ë¶€ë¥¼ ê²°ì •í•  ê²ƒì„ ê¶Œì¥í•©ë‹ˆë‹¤. í•™ìŠµ ì‹œê°„ì´ ë¶€ì¡±í•  ê²½ìš° ë³µì› íš¨ê³¼ê°€ í¬ê²Œ ë–¨ì–´ì§ˆ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

### ğŸ“Š íš¨ê³¼ í‰ê°€
mT5-base í‘œì¤€ ëª¨ë¸ê³¼ì˜ ì˜ˆë¹„ í…ŒìŠ¤íŠ¸ ë¹„êµ ê²°ê³¼:
* **í‘œì¤€ ëª¨ë¸ ì„±ëŠ¥**: ì „ë¬¸ ë„ë©”ì¸ ì–´íœ˜ ë³µì›ìœ¨ì´ 60% ì´í•˜ë¡œ ì¶”ì •ë˜ë©°, ë‚˜ë¨¸ì§€ 40%ì˜ ë³µì› ê²°ê³¼ëŠ” ë…¼ë¦¬ê°€ í˜¼ë€ìŠ¤ëŸ¬ì›Œ ì‹¤ì œ ì—…ë¬´ì— ì ìš©í•˜ê¸° ì–´ë ¤ìš´ ìˆ˜ì¤€ì…ë‹ˆë‹¤.
* **ë³¸ í”„ë¡œì íŠ¸ ê°œì„  í›„**: ì „ë¬¸ ì–´íœ˜ ë³µì›ìœ¨ì´ ì•½ 85%ì— ë„ë‹¬í–ˆìŠµë‹ˆë‹¤. ë‚˜ë¨¸ì§€ 15%ì˜ ì˜¤ì°¨ ëŒ€ë¶€ë¶„ë„ ì˜ë¯¸ê°€ ìœ ì‚¬í•œ ì–´íœ˜ë¡œ ëŒ€ì²´ëœ ê²ƒì´ì–´ì„œ, í…ìŠ¤íŠ¸ ì „ì²´ì˜ ê°€ë…ì„±ê³¼ ë…¼ë¦¬ì  ì¼ê´€ì„±ì´ í¬ê²Œ í–¥ìƒë˜ì—ˆìŠµë‹ˆë‹¤.

### âš ï¸ ì‚¬ìš© ì œí•œ
* **ë¬¸ë§¥ ë‹¨í¸í™” ì œí•œ**: ëª¨ë¸ì´ í•œ ë²ˆì— ì²˜ë¦¬í•  ìˆ˜ ìˆëŠ” í…ìŠ¤íŠ¸ ê¸¸ì´ì— í•œê³„ê°€ ìˆê³ , ê° í…ìŠ¤íŠ¸ êµ¬ê°„ ë‚´ ë§ˆìŠ¤í‚¹ë˜ëŠ” ì–´íœ˜ ìˆ˜ê°€ ì œí•œì ì´ì–´ì„œ, ê¸´ ë¬¸ì„œë¥¼ ë¶„í•  ì²˜ë¦¬í•  ë•Œ ë¬¸ë§¥ ì •ë³´ê°€ ëŠê²¨ êµ¬ê°„ì„ ë„˜ë‚˜ë“œëŠ” ì˜ë¯¸ë¥¼ ì™„ë²½í•˜ê²Œ í¬ì°©í•˜ì§€ ëª»í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì¼ë¶€ ë¬¸ë§¥ì„ í¬í•¨í•˜ì—¬ ì¬í•™ìŠµí•˜ëŠ” ê²ƒì„ ê¶Œì¥í•©ë‹ˆë‹¤.
* **ì•Œê³ ë¦¬ì¦˜ì˜ í•œê³„**: T5 ëª¨ë¸ ìì²´ì˜ ë³µì›ì€ í†µê³„ì  í™•ë¥  ì•Œê³ ë¦¬ì¦˜ì— ê¸°ë°˜í•˜ë¯€ë¡œ, ë³µì¡í•œ í…ìŠ¤íŠ¸ë¥¼ ì²˜ë¦¬í•  ë•Œ 100%ì˜ ë³µì› ì •í™•ë„ë¥¼ ë³´ì¥í•˜ëŠ” ê²ƒì€ ë¶ˆê°€ëŠ¥í•©ë‹ˆë‹¤.
* **ë„ë©”ì¸ ì˜ì¡´ì„±**: ë³µì› íš¨ê³¼ëŠ” ì‚¬ì „ì— ì„¤ì •ëœ ì „ë¬¸ê°€ ì‚¬ì „ì˜ ì»¤ë²„ë¦¬ì§€ì™€ ê¹Šì´ì— í¬ê²Œ ì˜ì¡´í•©ë‹ˆë‹¤.

### ğŸŒŒ í–¥í›„ ê°œë°œ ê³„íš
* ìë™ ê²°ì† ê°ì§€:
í† í°ë¼ì´ì €ì˜ 'ì´ìƒ íŒŒí¸'ì„ ì•”ì‹œì  ì‹ í˜¸ë¡œ í™œìš©í•©ë‹ˆë‹¤. OCR ì¸ì‹ì— ì‹¬ê°í•œ ì–´ê¸‹ë‚¨ì´ ë°œìƒí–ˆì„ ë•Œ, ëª¨ë¸ì´ í† í° ì‹œí€€ìŠ¤ì˜ ì´ìƒ ë³€ë™ì„ í†µí•´ ì˜ë¯¸ ë‹¨ì ˆ ë¶€ìœ„ë¥¼ ìë™ìœ¼ë¡œ ìœ„ì¹˜ íŒŒì•…í•  ìˆ˜ ìˆë„ë¡ í•©ë‹ˆë‹¤.
* ì˜ë¯¸ ìë™ ì •ë ¬:
ìˆ˜ë™ìœ¼ë¡œ ì—°ê²° ì§€ì ì„ ì§€ì •í•  í•„ìš” ì—†ì´, OCRë¡œ ì†ìƒëœ í…ìŠ¤íŠ¸ì— ëŒ€í•´ ëª¨ë¸ì´ ì—”ë“œ íˆ¬ ì—”ë“œ(End-to-End) ë³µêµ¬ë¥¼ ìˆ˜í–‰í•˜ë„ë¡ êµ¬í˜„í•©ë‹ˆë‹¤.

[Demo](#Demo)

---
<a name="portuguese"></a>
## PortuguÃªs (TraduÃ§Ã£o AutomÃ¡tica)
**T5-Refiner-DomainFocus** visa, atravÃ©s da otimizaÃ§Ã£o estratÃ©gica na **fase de prÃ©-treinamento**, conferir ao modelo uma "resiliÃªncia semÃ¢ntica" intrÃ­nseca, **permitindo que ele lide de forma mais robusta com a perda de texto e a injeÃ§Ã£o de conhecimento especializado de domÃ­nio.**

### ğŸ“– Contexto do Projeto
No processamento de **digitalizaÃ§Ã£o de arquivos mÃ©dicos**, o **OCR (Reconhecimento Ã“ptico de Caracteres)** frequentemente apresenta "defeitos de caracteres" em termos essenciais devido a danos no papel, obstruÃ§Ã£o por carimbos, entre outros motivos.
Os modelos tradicionais **T5** ou **mT5** (coletivamente chamados de T5) apresentam dois problemas principais ao lidar com esses textos danificados:
* **LimitaÃ§Ãµes do Mascaramento AleatÃ³rio**: Faz com que o modelo aprenda apenas a "adivinhar palavras" com base em radicais, sem entender verdadeiramente os conceitos mÃ©dicos completos.
* **Problema de Desalinhamento da TokenizaÃ§Ã£o**: Quando um termo perde letras, o tokenizador o fragmenta em pedaÃ§os sem sentido, fazendo com que o modelo perca o foco semÃ¢ntico.

### âœ… Principais Funcionalidades Atuais
Este projeto atualmente nÃ£o depende de regras de codificaÃ§Ã£o complexas, mas aprimora a capacidade do modelo por meio da otimizaÃ§Ã£o do prÃ©-processamento de dados, das estratÃ©gias de mÃ¡scara e do fluxo de execuÃ§Ã£o:

**Nota:** Este projeto utiliza o prÃ³prio tokenizer do T5.

* **DivisÃ£o de textos longos em blocos de treinamento por parÃ¡grafo e pontuaÃ§Ã£o**  
  ConstrÃ³i automaticamente amostras a partir de romances, documentos ou corpora especializados, dividindo com base em limites de parÃ¡grafos e pontuaÃ§Ã£o em chinÃªs e inglÃªs, ao mesmo tempo em que mantÃ©m a frase final do bloco anterior como prefixo contextual, garantindo a continuidade semÃ¢ntica das amostras, mitigando quebras de sentido entre blocos e controlando o tamanho das amostras para se adequar Ã  janela de contexto do modelo.

* **MÃ¡scara atomizada guiada por dicionÃ¡rio de termos especializados**  
  Utiliza um dicionÃ¡rio personalizado de termos de domÃ­nio para tratar termos tÃ©cnicos (por exemplo: infarto agudo de parede anterior, intervenÃ§Ã£o coronÃ¡ria percutÃ¢nea) como unidades indivisÃ­veis durante a aplicaÃ§Ã£o da mÃ¡scara, bloqueando caminhos de previsÃ£o por caractere e forÃ§ando o modelo a reconstruir o conteÃºdo com base na lÃ³gica e semÃ¢ntica do contexto.

* **EstratÃ©gia de mÃ¡scara reforÃ§ada e controlÃ¡vel manualmente**  
  Permite aumentar manualmente a probabilidade de mascaramento de termos de alta complexidade (**ğŸ’¡ recomendado entre 50%â€“70%, nÃ£o ultrapassar 80%**) e ajustar simultaneamente a taxa total de mÃ¡scara (aproximadamente 20%â€“25%), fortalecendo a capacidade de raciocÃ­nio do modelo em pontos fracos do conhecimento.

* **Evitando automaticamente pontuaÃ§Ã£o e tokens semÃ¢nticos irrelevantes**  
  Durante o mascaramento, ignora automaticamente pontuaÃ§Ãµes em chinÃªs e inglÃªs, sÃ­mbolos e caracteres desconhecidos do tokenizer (`<unk>`), garantindo que cada span mascarado corresponda a uma unidade semÃ¢ntica vÃ¡lida.

* **Mesclagem automÃ¡tica de spans consecutivos ou sobrepostos**  
  Mescla spans de mÃ¡scara adjacentes ou sobrepostos automaticamente, garantindo que os pares de entrada/saÃ­da `<extra_id_n>` sejam ordenados corretamente, com sinal de treinamento contÃ­nuo, totalmente compatÃ­veis com a norma de span corruption do artigo original do T5.

* **Processamento paralelo multithread e retorno automÃ¡tico das amostras**  
  Suporta a geraÃ§Ã£o de amostras em paralelo com nÃºmero de threads configurÃ¡vel, aumentando significativamente a eficiÃªncia do processamento de grandes corpora; todas as amostras geradas sÃ£o retornadas automaticamente e reorganizadas na ordem original, resultando em um dataset JSONL padronizado pronto para treinamento.

**Por meio da construÃ§Ã£o de dados sensÃ­veis ao contexto, mascaramento priorizado por palavras-chave, spans aleatÃ³rios e cenÃ¡rios de "perda extrema de informaÃ§Ã£o" artificialmente criados, o modelo mantÃ©m a compreensÃ£o e reconstruÃ§Ã£o precisa da semÃ¢ntica especializada mesmo nas condiÃ§Ãµes de entrada mais adversas.**

> CenÃ¡rios de aplicaÃ§Ã£o: corpora mÃ©dicos, jurÃ­dicos, documentos tÃ©cnicos ou qualquer contexto que exija compreensÃ£o precisa da semÃ¢ntica especializada.

### â—ï¸ ObservaÃ§Ãµes de Treinamento
* **PrevenÃ§Ã£o de Parada Precoce**: ApÃ³s o prÃ©-processamento, o modelo T5 pode apresentar uma queda lenta na Loss ou flutuaÃ§Ãµes locais ilusÃ³rias, levando o sistema a interromper o treinamento prematuramente por erro.
* **SugestÃ£o de Julgamento de ConvergÃªncia**: Recomenda-se aumentar o tempo de treinamento e julgar a convergÃªncia do modelo de forma abrangente, baseando-se na estabilidade da queda da Loss em mÃºltiplas etapas. Se o tempo de treinamento for insuficiente, o efeito de restauraÃ§Ã£o pode ser drasticamente reduzido.

### ğŸ“Š AvaliaÃ§Ã£o de Resultados
De acordo com testes comparativos preliminares no modelo padrÃ£o mT5-base:
* **Desempenho do Modelo PadrÃ£o**: A taxa de restauraÃ§Ã£o de vocabulÃ¡rio especializado Ã© estimada em menos de 60%, com os 40% restantes apresentando resultados logicamente confusos, sendo quase inaceitÃ¡veis para o negÃ³cio.
* **ApÃ³s Melhorias Deste Projeto**: A taxa de restauraÃ§Ã£o de vocabulÃ¡rio especializado atingiu estimadamente 85%. Nos 15% de erro restantes, a maioria sÃ£o substituiÃ§Ãµes por termos semanticamente prÃ³ximos, melhorando significativamente a legibilidade geral e a coerÃªncia lÃ³gica do texto.

### âš ï¸ LimitaÃ§Ãµes de Uso
* **RestriÃ§Ã£o de FragmentaÃ§Ã£o de Contexto**: Devido ao limite de extensÃ£o de texto processado por vez e ao nÃºmero limitado de termos mascarados em cada segmento, documentos longos podem sofrer quebras de informaÃ§Ã£o contextual durante o corte, impedindo a captura perfeita de semÃ¢nticas que cruzam parÃ¡grafos. Recomenda-se reenviar parte do contexto para re-treinamento.
* **LimitaÃ§Ãµes do Algoritmo**: Como a restauraÃ§Ã£o do prÃ³prio modelo T5 Ã© baseada em algoritmos de probabilidade estatÃ­stica, Ã© impossÃ­vel garantir 100% de precisÃ£o na restauraÃ§Ã£o de textos complexos.
* **DependÃªncia de DomÃ­nio**: O efeito de restauraÃ§Ã£o depende altamente da abrangÃªncia e profundidade do glossÃ¡rio especializado predefinido.

### ğŸŒŒ Plano de Desenvolvimento Futuro
* PercepÃ§Ã£o AutomÃ¡tica de Defeitos:
Utilizar "fragmentos anÃ´malos" do tokenizador como sinais implÃ­citos. Quando o reconhecimento de OCR apresentar desalinhamentos graves, o modelo poderÃ¡ localizar automaticamente a quebra semÃ¢ntica atravÃ©s das flutuaÃ§Ãµes anormais na sequÃªncia de tokens.
* Alinhamento SemÃ¢ntico AutomÃ¡tico:
Realizar a reparaÃ§Ã£o ponta a ponta de textos danificados por OCR sem a necessidade de especificar manualmente os pontos de conexÃ£o.

[Demo](#Demo)

---
<a name="Demo"></a>
## ğŸ“¡ Demo

-[Demo-EN](./Demo-EN.txt)

ï¼ˆMost of the application environment for this project is in Chinese, which may cause minor issues with processing English, such as splitting by punctuation. The T5 model itself does support English.ï¼‰

- â­ï¸[Demo-CN](./Demo-CN.txt)

[Introduction](#Introduction)
  
---
<a name="Requirements"></a>
## ğŸ› ï¸ Requirements

```text
transformers>=4.30.0  # HuggingFace Transformers
regex                
tqdm                
```
---
<a name="References"></a>
## ğŸ’ªReferences / Citation
```markdown
This project builds upon the T5 or mT5. If you use mT5, please cite:

@inproceedings{xue-etal-2021-mt5,
    title = "m{T}5: A Massively Multilingual Pre-trained Text-to-Text Transformer",
    author = "Xue, Linting  and
      Constant, Noah  and
      Roberts, Adam  and
      Kale, Mihir  and
      Al-Rfou, Rami  and
      Siddhant, Aditya  and
      Barua, Aditya  and
      Raffel, Colin",
    booktitle = "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
    month = jun,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.naacl-main.41",
    doi = "10.18653/v1/2021.naacl-main.41",
    pages = "483--498"
}

If you use this project, please cite it as:

@misc{llap4585,
    title={{T5-Refiner-DomainFocus}: Injecting domain expertise into T5 via precision vocabulary-guided masking.},
    author={llap4585},
    howpublished = {\url{https://github.com/llap4585/T5-Refiner-DomainFocus}},
    year={2026}
}

```
---

<a name="Privacy"></a>
## ğŸ›¡ï¸ Privacy & Security

**Local Processing Only:** This tool performs all operations locally on your machine. No medical reports, patient data, or sensitive information are uploaded to any external servers or cloud services. Your data remains under your control at all times.

**Third-party Disclaimer:** All third-party libraries required for operation are provided by the user's environment. These dependencies and their components are not under the management or control of this project.

**ä»…é™æœ¬åœ°å¤„ç†ï¼š** æœ¬å·¥å…·çš„æ‰€æœ‰æ“ä½œå‡åœ¨æ‚¨çš„æœ¬åœ°è®¡ç®—æœºä¸Šæ‰§è¡Œã€‚ä¸ä¼šå°†ä»»ä½•åŒ»ç–—æŠ¥å‘Šã€æ‚£è€…æ•°æ®æˆ–æ•æ„Ÿä¿¡æ¯ä¸Šä¼ åˆ°ä»»ä½•å¤–éƒ¨æœåŠ¡å™¨æˆ–äº‘æœåŠ¡ã€‚æ‚¨çš„æ•°æ®å§‹ç»ˆç”±æ‚¨æŒæ§ã€‚

**ç¬¬ä¸‰æ–¹åº“å£°æ˜ï¼š** æœ¬å·¥å…·è¿è¡Œæ‰€ä¾èµ–çš„æ‰€æœ‰ç¬¬ä¸‰æ–¹åº“å‡ç”±ç”¨æˆ·ç¯å¢ƒæä¾›ï¼Œè¿™äº›ç¬¬ä¸‰æ–¹åº“åŠå…¶ç›¸å…³ç»„ä»¶ä¸åœ¨æœ¬é¡¹ç›®çš„ç®¡ç†ä¸æ§åˆ¶èŒƒå›´å†…ã€‚


---
> **âš ï¸Disclaimer:** The non-English and non-Chinese versions of this documentation are provided for convenience only and were generated using machine translation. README may have been revised multiple times, and non-Chinese content may be missing. In case of any discrepancy, the Chinese version shall prevail. 
