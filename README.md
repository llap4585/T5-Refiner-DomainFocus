# T5-Refiner-DomainFocus
# T5-é¢„å¤„ç†ç‰¹å®šè¯è¯­é®ç…§å¢å¼º

![Views](https://komarev.com/ghpvc/?username=llap4585&repo=T5-Refiner-DomainFocus&label=Project%20Views&color=blue&style=flat-square)

---
[â­ï¸English](#english) | [â­ï¸ä¸­æ–‡](#chinese)


[æ—¥æœ¬èª](#japanese) | [Deutsch](#deutsch) | [FranÃ§ais](#francais) | [EspaÃ±ol](#espanol) | [à¤¹à¤¿à¤¨à¥à¤¦à¥€](#hindi) | [í•œêµ­ì–´](#korean) | [PortuguÃªs](#portuguese)

---

[Demo](#Demo) 

[Requirements](#Requirements)

[References](#References)

[Privacy](#Privacy)

---
<a name="english"></a>
## â­ï¸ English

T5-Refiner-DomainFocus aims to empower models with intrinsic "semantic resilience" through pre-training stage strategy optimization, enabling more robust handling of text corruption and the injection of domain-specific expertise.

### ğŸ“– Project Background
During the digitization of medical records, OCR (Optical Character Recognition) often suffers from "character defects" in core terminology due to damaged paper, stamp occlusion, or other physical factors.
Traditional T5 or mT5 models (collectively referred to as T5) face two major challenges when processing such corrupted text:
* Limitations of Random Masking: The model learns to "guess" words based on sub-word roots rather than truly understanding complete medical concepts.
* Tokenization Misalignment: When letters are missing from a term, the tokenizer breaks it into meaningless fragments, causing the model to lose its semantic focus.

### âœ… Core Features
This project enhances model capability by optimizing the data preprocessing pipeline rather than relying on complex hard-coded rules:

* Expert Lexicon-Guided Atomic Masking:
By leveraging custom lexicons, the model is forced to treat professional terms (e.g., Acute Anterior Myocardial Infarction) as indivisible units during masking. This forces the model to derive answers from contextual logic rather than taking shortcuts via residual characters.

* Manual Enhanced Training:
Supports manual adjustment of masking probabilities for specific high-difficulty terms (ğŸ’¡Recommended: 50%-70%, not to exceed 80%), while simultaneously increasing the global masking rate (20%-25%).

* Automatic Punctuation Avoidance:
Prevents the introduction of noise interference during the masking process.

By creating scenarios of "extreme information loss," the model is compelled to maintain accurate reconstruction of professional semantics even under the worst input conditions.

### â—ï¸ Training Notes
* Preventing Early Stopping: After preprocessing, T5 models may exhibit slow loss reduction or local fluctuations, which can trick systems into stopping training prematurely.
* Convergence Judgment: It is recommended to extend training duration and evaluate convergence based on whether the loss decreases steadily across multiple stages. Insufficient training will significantly degrade restoration performance.

### ğŸ“Š Evaluation
Based on preliminary testing with the mT5-base standard model:
* Standard Model Performance: The restoration rate for specialized terminology is estimated to be below 60%. The remaining 40% of results are often logically incoherent and unacceptable for professional use.
* With DomainFocus Improvement: The estimated restoration rate reaches 85%. Of the remaining 15% error margin, most are semantic synonyms, which greatly improves the overall readability and logical consistency of the text.

### âš ï¸ Limitations
* Context Fragmentation: Due to the limited sequence length and the restricted number of masks per segment, long documents may suffer from semantic disconnection during chunking. It is recommended to feed partial overlapping context back during re-training.
* Algorithmic Limits: Since T5 restoration is based on statistical probability, it is impossible to guarantee 100% accuracy when dealing with highly complex text.
* Domain Dependency: The restoration effectiveness is highly dependent on the coverage and depth of the predefined expert lexicon.

### ğŸŒŒ Future Roadmap
* Automatic Defect Sensing:
Utilizing "tokenization fragments" as implicit signals. When OCR recognition is severely misaligned, the model will automatically locate semantic ruptures via anomalies in the token sequence.
* Semantic Auto-Alignment:
Eliminating the need for manual anchor points to achieve end-to-end restoration of OCR-damaged text.

[Demo](#Demo)

---
<a name="chinese"></a>
## â­ï¸ä¸­æ–‡
**T5-Refiner-DomainFocus** æ—¨åœ¨é€šè¿‡**é¢„è®­ç»ƒé˜¶æ®µ**çš„ç­–ç•¥ä¼˜åŒ–ï¼Œèµ‹äºˆæ¨¡å‹ä¸€ç§å†…åœ¨çš„â€œè¯­ä¹‰éŸ§æ€§â€ï¼Œä½¿å…¶èƒ½æ›´ç¨³å¥åœ°å¤„ç†æ–‡æœ¬ç¼ºæŸå’Œæ³¨å…¥é¢†åŸŸä¸“ä¸šçŸ¥è¯†ã€‚

### ğŸ“–é¡¹ç›®èƒŒæ™¯
åœ¨å¤„ç†**åŒ»å­¦æ¡£æ¡ˆæ•°å­—åŒ–æ—¶**ï¼Œ**OCRï¼ˆå…‰å­¦å­—ç¬¦è¯†åˆ«ï¼‰** å¸¸å› çº¸è´¨å—æŸã€å°ç« é®æŒ¡ç­‰åŸå› ï¼Œå¯¼è‡´æ ¸å¿ƒæœ¯è¯­å‡ºç°â€œå­—ç¬¦ç¼ºæŸâ€ã€‚
ä¼ ç»Ÿçš„ **T5** æˆ– **mT5** æ¨¡å‹(ç»Ÿç§°T5ï¼‰åœ¨å¤„ç†è¿™äº›å—æŸæ–‡æœ¬æ—¶å­˜åœ¨ä¸¤ä¸ªä¸»è¦é—®é¢˜ï¼š
* éšæœºé®è”½çš„å±€é™æ€§ï¼šå¯¼è‡´æ¨¡å‹åªå­¦ä¼šäº†æ ¹æ®è¯æ ¹â€œçŒœè¯â€ï¼Œè€Œæ²¡æœ‰çœŸæ­£ç†è§£å®Œæ•´çš„åŒ»å­¦æ¦‚å¿µã€‚
* åˆ†è¯é”™ä½é—®é¢˜ï¼šå½“æœ¯è¯­ä¸¢å¤±å­—æ¯æ—¶ï¼Œåˆ†è¯å™¨ä¼šå°†å…¶åˆ‡ç¢ä¸ºæ— æ„ä¹‰çš„ç¢ç‰‡ï¼Œå¯¼è‡´æ¨¡å‹å¤±å»è¯­ä¹‰é‡å¿ƒã€‚

### âœ…å½“å‰æ ¸å¿ƒåŠŸèƒ½
æœ¬é¡¹ç›®ç›®å‰ä¸ä¾èµ–å¤æ‚çš„ç¡¬ç¼–ç è§„åˆ™ï¼Œè€Œæ˜¯é€šè¿‡ä¼˜åŒ–æ•°æ®é¢„å¤„ç†æµç¨‹æ¥å¢å¼ºæ¨¡å‹èƒ½åŠ›ï¼š

* ä¸“å®¶è¯åº“å¼•å¯¼çš„åŸå­åŒ–é®è”½ï¼š
ä¾æ‰˜è‡ªå®šä¹‰è¯åº“ï¼Œå¼ºåˆ¶æ¨¡å‹å°†ä¸“ä¸šæœ¯è¯­ï¼ˆå¦‚ï¼šæ€¥æ€§å‰å£å¿ƒè‚Œæ¢—æ­»ï¼‰è§†ä¸ºä¸å¯åˆ†å‰²çš„æ•´ä½“è¿›è¡Œé®è”½ã€‚é€šè¿‡è¿™ç§æ–¹å¼ï¼Œè¿«ä½¿æ¨¡å‹ä»ä¸Šä¸‹æ–‡çš„é€»è¾‘ä¸­å¯»æ‰¾ç­”æ¡ˆï¼Œè€Œéé€šè¿‡æ®‹ä½™å­—ç¬¦æŠ•æœºå–å·§ã€‚

* äººå·¥è®¾å®šå¼ºåŒ–è®­ç»ƒï¼š
æ”¯æŒæ‰‹åŠ¨æé«˜ç‰¹å®šé«˜éš¾åº¦æœ¯è¯­çš„é®è”½æ¦‚ç‡ï¼ˆğŸ’¡æ¨èåœ¨50%-70%ï¼Œä¸å®œè¶…è¿‡80%ï¼‰ï¼ŒåŒæ—¶å¯åŒæ­¥æé«˜æ•´ä½“çš„é®è”½ç‡ï¼ˆ20%-25%ï¼‰ã€‚

* è‡ªåŠ¨è§„é¿æ ‡ç‚¹ç¬¦å·ï¼š
é˜²æ­¢å¼•å…¥å¹²æ‰°ã€‚

é€šè¿‡äººä¸ºåˆ¶é€ â€œæç«¯ä¿¡æ¯ç¼ºå¤±â€çš„åœºæ™¯ï¼Œå¼ºåˆ¶æ¨¡å‹åœ¨æœ€å·®çš„è¾“å…¥æƒ…å†µä¸‹ä¾ç„¶èƒ½ä¿æŒå¯¹ä¸“ä¸šè¯­ä¹‰çš„å‡†ç¡®è¿˜åŸã€‚

### â—ï¸è®­ç»ƒæ³¨æ„äº‹é¡¹
* é˜²æ­¢æ¨¡å‹æå‰åœæ­¢ï¼šåœ¨é¢„å¤„ç†ä¹‹åï¼ŒT5 æ¨¡å‹å¯èƒ½ä¼šå‡ºç° Loss ä¸‹é™ç¼“æ…¢æˆ–äº§ç”Ÿå±€éƒ¨æ³¢åŠ¨çš„å‡è±¡ï¼Œå¯¼è‡´ç³»ç»Ÿé”™è¯¯åœ°æå‰åœæ­¢è®­ç»ƒã€‚
* æ”¶æ•›åˆ¤æ–­å»ºè®®ï¼šæ¨èå¢åŠ è®­ç»ƒæ—¶é•¿ï¼Œå¹¶æ ¹æ®å¤šä¸ªé˜¶æ®µçš„ Loss æ˜¯å¦æŒç»­ç¨³å®šä¸‹é™æ¥ç»¼åˆåˆ¤æ–­æ¨¡å‹æ”¶æ•›æƒ…å†µã€‚è‹¥è®­ç»ƒæ—¶é—´ä¸è¶³ï¼Œè¿˜åŸæ•ˆæœå¯èƒ½ä¼šå¤§æ‰“æŠ˜æ‰£ã€‚

### ğŸ“Šæ•ˆæœè¯„ä¼°
æ ¹æ®åˆæ­¥æµ‹è¯•å¯¹æ¯”ï¼Œåœ¨ mT5-base æ ‡å‡†æ¨¡å‹ä¸­ï¼š
* æ ‡å‡†æ¨¡å‹è¡¨ç°ï¼šåœ¨ä¸“ä¸šé¢†åŸŸçš„è¯æ±‡è¿˜åŸç‡ä¼°ç®—åœ¨ 60% ä»¥ä¸‹ï¼Œå‰©ä½™ 40% çš„è¿˜åŸç»“æœé€»è¾‘æ··ä¹±ï¼Œå‡ ä¹æ— æ³•è¢«ä¸šåŠ¡æ¥å—ã€‚
* æœ¬é¡¹ç›®æ”¹è¿›åï¼šä¸“ä¸šè¯æ±‡è¿˜åŸç‡ä¼°ç®—è¾¾åˆ°äº† 85%ã€‚å‰©ä¸‹çš„ 15% è¯¯å·®ä¸­ï¼Œå¤§éƒ¨åˆ†æ˜¯è¯­ä¹‰ç›¸è¿‘çš„è¯æ±‡æ›¿ä»£ï¼Œæå¤§åœ°æé«˜äº†æ–‡æœ¬çš„æ•´ä½“å¯è¯»æ€§å’Œé€»è¾‘è¿è´¯æ€§ã€‚

### âš ï¸ä½¿ç”¨é™åˆ¶
* ä¸Šä¸‹æ–‡ç‰‡æ®µåŒ–é™åˆ¶ï¼šç”±äºæ¨¡å‹å•æ¬¡å¤„ç†çš„æ–‡æœ¬é•¿åº¦æœ‰é™ï¼Œä¸”æ¯æ®µæ–‡æœ¬å†…æ ‡è®°ï¼ˆMaskï¼‰çš„è¯æ±‡æ•°é‡å—é™ï¼Œé•¿æ–‡æ¡£åœ¨åˆ‡åˆ†å¤„ç†æ—¶å¯èƒ½å­˜åœ¨ä¸Šä¸‹æ–‡ä¿¡æ¯æ–­è£‚çš„æƒ…å†µï¼Œå¯¼è‡´éƒ¨åˆ†è·¨æ®µè½çš„è¯­ä¹‰æ— æ³•è¢«å®Œç¾æ•æ‰ã€‚æ¨èå›ä¼ éƒ¨åˆ†ä¸Šä¸‹æ–‡å†è®­ç»ƒã€‚
* ç®—æ³•å±€é™æ€§ï¼šç”±äº T5 æ¨¡å‹æœ¬èº«çš„è¿˜åŸæ˜¯åŸºäºç»Ÿè®¡æ¦‚ç‡ç®—æ³•çš„ï¼Œå› æ­¤åœ¨å¤„ç†å¤æ‚çš„æ–‡æœ¬æ—¶ï¼Œä¸å¯èƒ½ä¿è¯ 100% çš„è¿˜åŸå‡†ç¡®ç‡ã€‚
* é¢†åŸŸä¾èµ–ï¼šè¿˜åŸæ•ˆæœé«˜åº¦ä¾èµ–äºé¢„è®¾ä¸“å®¶è¯åº“çš„è¦†ç›–é¢ä¸æ·±åº¦ã€‚

### ğŸŒŒæœªæ¥å¼€å‘è®¡åˆ’
* è‡ªåŠ¨ç¼ºæŸæ„ŸçŸ¥:
åˆ©ç”¨åˆ†è¯å™¨çš„â€œå¼‚å¸¸ç¢ç‰‡â€ä½œä¸ºéšæ€§ä¿¡å·ã€‚å½“ OCR è¯†åˆ«å‡ºç°ä¸¥é‡é”™ä½æ—¶ï¼Œæ¨¡å‹èƒ½é€šè¿‡åˆ†è¯åºåˆ—çš„å¼‚å¸¸æ³¢åŠ¨ï¼Œè‡ªåŠ¨å®šä½åˆ°è¯­ä¹‰æ–­è£‚å¤„ã€‚
* è¯­ä¹‰è‡ªåŠ¨å¯¹é½:
æ— éœ€äººå·¥æŒ‡å®šè¡”æ¥ç‚¹ï¼Œå®ç°æ¨¡å‹å¯¹ OCR æŸåæ–‡æœ¬çš„ç«¯åˆ°ç«¯ä¿®å¤ã€‚

[Demo](#Demo) 

---
<a name="Demo"></a>
## ğŸ“¡ Demo

---
<a name="Requirements"></a>
## ğŸ› ï¸ Requirements

```text


```
---
<a name="References"></a>
## ğŸ’ªReferences / Citation
```markdown
This project builds upon the T5 or mT5. If you use mT5, please cite:

@inproceedings{xue-etal-2021-mt5,
    title = "m{T}5: A Massively Multilingual Pre-trained Text-to-Text Transformer",
    author = "Xue, Linting  and
      Constant, Noah  and
      Roberts, Adam  and
      Kale, Mihir  and
      Al-Rfou, Rami  and
      Siddhant, Aditya  and
      Barua, Aditya  and
      Raffel, Colin",
    booktitle = "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
    month = jun,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.naacl-main.41",
    doi = "10.18653/v1/2021.naacl-main.41",
    pages = "483--498"
}

If you use this project, please cite it as:

@misc{llap4585,
    title={{T5-Refiner-DomainFocus}: Injecting domain expertise into T5 via precision vocabulary-guided masking.},
    author={llap4585},
    howpublished = {\url{https://github.com/llap4585/T5-Refiner-DomainFocus}},
    year={2026}
}

```
---

<a name="Privacy"></a>
## ğŸ›¡ï¸ Privacy & Security

**Local Processing Only:** This tool performs all operations locally on your machine. No medical reports, patient data, or sensitive information are uploaded to any external servers or cloud services. Your data remains under your control at all times.

**Third-party Disclaimer:** All third-party libraries required for operation are provided by the user's environment. These dependencies and their components are not under the management or control of this project.

**ä»…é™æœ¬åœ°å¤„ç†ï¼š** æœ¬å·¥å…·çš„æ‰€æœ‰æ“ä½œå‡åœ¨æ‚¨çš„æœ¬åœ°è®¡ç®—æœºä¸Šæ‰§è¡Œã€‚ä¸ä¼šå°†ä»»ä½•åŒ»ç–—æŠ¥å‘Šã€æ‚£è€…æ•°æ®æˆ–æ•æ„Ÿä¿¡æ¯ä¸Šä¼ åˆ°ä»»ä½•å¤–éƒ¨æœåŠ¡å™¨æˆ–äº‘æœåŠ¡ã€‚æ‚¨çš„æ•°æ®å§‹ç»ˆç”±æ‚¨æŒæ§ã€‚

**ç¬¬ä¸‰æ–¹åº“å£°æ˜ï¼š** æœ¬å·¥å…·è¿è¡Œæ‰€ä¾èµ–çš„æ‰€æœ‰ç¬¬ä¸‰æ–¹åº“å‡ç”±ç”¨æˆ·ç¯å¢ƒæä¾›ï¼Œè¿™äº›ç¬¬ä¸‰æ–¹åº“åŠå…¶ç›¸å…³ç»„ä»¶ä¸åœ¨æœ¬é¡¹ç›®çš„ç®¡ç†ä¸æ§åˆ¶èŒƒå›´å†…ã€‚


---
> **âš ï¸Disclaimer:** The non-English and non-Chinese versions of this documentation are provided for convenience only and were generated using machine translation. In case of any discrepancy, the Chinese version shall prevail.
